{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. í•„ìš” íŒŒì¼ ëª©ë¡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 'nst_{fish}_trend_2025-01-30.csv'\n",
    "* 'forecast_agg.csv'\n",
    "* 'ikh_item_price_2025-01-30.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ê²½ì œì§€í‘œ ë°ì´í„° ì „ì²˜ë¦¬\n",
    "\n",
    "filled_economic_indicators.csv ìƒì„±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-1. yfinanceì—ì„œ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "\n",
    "* ì…ë ¥ íŒŒì¼ :  ì—†ìŒ\n",
    "* ì¶œë ¥ íŒŒì¼ : '_economic_indicators_.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First few rows of the data:\n",
      "                USD/KRW        KOSPI        WTI        VIX         Gold  \\\n",
      "Date                                                                      \n",
      "2015-01-01  1092.699951          NaN        NaN        NaN          NaN   \n",
      "2015-01-02  1093.599976  1926.439941  52.689999  17.790001  1186.000000   \n",
      "2015-01-05  1111.000000  1915.750000  50.040001  19.920000  1203.900024   \n",
      "2015-01-06  1108.500000  1882.449951  47.930000  21.120001  1219.300049   \n",
      "2015-01-07  1097.300049  1883.829956  48.650002  19.309999  1210.599976   \n",
      "\n",
      "               Silver       MOVE  \n",
      "Date                              \n",
      "2015-01-01        NaN        NaN  \n",
      "2015-01-02  15.734000  70.000000  \n",
      "2015-01-05  16.179001  71.500000  \n",
      "2015-01-06  16.603001  84.900002  \n",
      "2015-01-07  16.510000  85.199997  \n",
      "\n",
      "Basic statistics:\n",
      "           USD/KRW        KOSPI          WTI          VIX         Gold  \\\n",
      "count  2626.000000  2625.000000  2625.000000  2625.000000  2625.000000   \n",
      "mean   1198.244361  2373.886004    62.117570    18.236381  1611.870780   \n",
      "std      95.503176   350.355287    18.080615     7.224219   403.104126   \n",
      "min    1053.729980  1457.640015   -37.630001     9.140000  1050.800049   \n",
      "25%    1125.462463  2070.540039    48.660000    13.340000  1263.900024   \n",
      "50%    1172.719971  2360.810059    60.139999    16.320000  1550.400024   \n",
      "75%    1272.287537  2571.090088    74.110001    21.280001  1875.400024   \n",
      "max    1473.520020  3305.209961   123.699997    82.690002  2788.500000   \n",
      "\n",
      "            Silver         MOVE  \n",
      "count  2625.000000  2625.000000  \n",
      "mean     20.172996    79.393345  \n",
      "std       4.835867    28.073940  \n",
      "min      11.735000    36.619999  \n",
      "25%      16.299000    56.220001  \n",
      "50%      18.097000    71.860001  \n",
      "75%      23.896000    98.730003  \n",
      "max      34.831001   182.639999  \n",
      "\n",
      "Data saved to _economic_indicators_.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_39148\\280239614.py:33: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_final = df_final.fillna(method='ffill')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import yfinance as yf\n",
    "\n",
    "# ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ ì„¤ì •\n",
    "start_date = '2015-01-01'\n",
    "end_date = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# ë°ì´í„°ë¥¼ ì €ì¥í•  ë¹ˆ ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "df_final = pd.DataFrame()\n",
    "\n",
    "# ê° ì§€í‘œì˜ í‹°ì»¤ ì‹¬ë³¼ ì •ì˜\n",
    "tickers = {\n",
    "    'USD/KRW': 'KRW=X',  # ì›ë‹¬ëŸ¬ í™˜ìœ¨\n",
    "    'KOSPI': '^KS11',  # KOSPI\n",
    "    'WTI': 'CL=F',  # WTI ì›ìœ  ì„ ë¬¼\n",
    "    'VIX': '^VIX',  # VIX ì§€ìˆ˜\n",
    "    'Gold': 'GC=F',  # ê¸ˆ ì„ ë¬¼\n",
    "    'Silver': 'SI=F',  # ì€ ì„ ë¬¼\n",
    "    'MOVE' : '^MOVE' # MOVE Index\n",
    "}\n",
    "\n",
    "# ê° í‹°ì»¤ì— ëŒ€í•´ ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n",
    "for name, ticker in tickers.items():\n",
    "    try:\n",
    "        df = yf.download(ticker, start=start_date, end=end_date, interval=\"1d\", progress=False)\n",
    "        # ì¢…ê°€ë§Œ ì‚¬ìš©\n",
    "        df_final[name] = df['Close']\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {name}: {e}\")\n",
    "\n",
    "# ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
    "df_final = df_final.fillna(method='ffill')\n",
    "\n",
    "# ë°ì´í„° í™•ì¸\n",
    "print(\"\\nFirst few rows of the data:\")\n",
    "print(df_final.head())\n",
    "\n",
    "# ê¸°ë³¸ í†µê³„ í™•ì¸\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(df_final.describe())\n",
    "\n",
    "# CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "output_filename = '_economic_indicators_.csv'\n",
    "df_final.to_csv(output_filename)\n",
    "print(f\"\\nData saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-2. ê²½ì œì§€í‘œ ë°ì´í„° ê²°ì¸¡ì¹˜ ì±„ìš°ê¸° \n",
    "\n",
    "* ì…ë ¥ íŒŒì¼ : '_economic_indicators_.csv'\n",
    "* ì¶œë ¥ íŒŒì¼ : 'filled_economic_indicators_.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì²˜ë¦¬ëœ ë°ì´í„° ìƒ˜í”Œ:\n",
      "        Date      USD/KRW        KOSPI        WTI        VIX         Gold  \\\n",
      "0 2015-01-01  1092.699951          NaN        NaN        NaN          NaN   \n",
      "1 2015-01-02  1093.599976  1926.439941  52.689999  17.790001  1186.000000   \n",
      "2 2015-01-03  1093.599976  1926.439941  52.689999  17.790001  1186.000000   \n",
      "3 2015-01-04  1093.599976  1926.439941  52.689999  17.790001  1186.000000   \n",
      "4 2015-01-05  1111.000000  1915.750000  50.040001  19.920000  1203.900024   \n",
      "\n",
      "      Silver  MOVE  \n",
      "0        NaN   NaN  \n",
      "1  15.734000  70.0  \n",
      "2  15.734000  70.0  \n",
      "3  15.734000  70.0  \n",
      "4  16.179001  71.5  \n",
      "\n",
      "ê²°ì¸¡ì¹˜ í™•ì¸:\n",
      "Date       0\n",
      "USD/KRW    0\n",
      "KOSPI      1\n",
      "WTI        1\n",
      "VIX        1\n",
      "Gold       1\n",
      "Silver     1\n",
      "MOVE       1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "df = pd.read_csv('_economic_indicators_.csv', encoding='utf-8')\n",
    "\n",
    "# Date ì»¬ëŸ¼ì„ datetimeìœ¼ë¡œ ë³€í™˜\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# ì‹œì‘ì¼ê³¼ ë§ˆì§€ë§‰ì¼ ì¶”ì¶œ\n",
    "start_date = df['Date'].min()\n",
    "end_date = df['Date'].max()\n",
    "\n",
    "# ëª¨ë“  ë‚ ì§œê°€ í¬í•¨ëœ ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "all_dates = pd.DataFrame(\n",
    "    {'Date': pd.date_range(start_date, end_date, freq='D')}\n",
    ")\n",
    "\n",
    "# ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•©\n",
    "filled_df = pd.merge(all_dates, df, on='Date', how='left')\n",
    "\n",
    "# ê²°ì¸¡ì¹˜ë¥¼ ì´ì „ ê°’ìœ¼ë¡œ ì±„ìš°ê¸°\n",
    "filled_df = filled_df.ffill()\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "filled_df.to_csv('filled_economic_indicators.csv', index=False)\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸\n",
    "print(\"ì²˜ë¦¬ëœ ë°ì´í„° ìƒ˜í”Œ:\")\n",
    "print(filled_df.head())\n",
    "print(\"\\nê²°ì¸¡ì¹˜ í™•ì¸:\")\n",
    "print(filled_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-3. ì–´ì¢…ë³„ ê²½ì œì§€í‘œ ì¹¼ëŸ¼ ìƒì„±\n",
    "\n",
    "* ì…ë ¥ íŒŒì¼ : 'filled_economic_indicators_.csv'\n",
    "* ì¶œë ¥ íŒŒì¼ : 'expanded_economic_indicators_.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… ë³€ê²½ëœ ë°ì´í„° (ìƒìœ„ 5ê°œ í–‰):\n",
      "         Date      USD/KRW        KOSPI        WTI        VIX         Gold  \\\n",
      "0  2015-01-01  1092.699951          NaN        NaN        NaN          NaN   \n",
      "1  2015-01-02  1093.599976  1926.439941  52.689999  17.790001  1186.000000   \n",
      "2  2015-01-03  1093.599976  1926.439941  52.689999  17.790001  1186.000000   \n",
      "3  2015-01-04  1093.599976  1926.439941  52.689999  17.790001  1186.000000   \n",
      "4  2015-01-05  1111.000000  1915.750000  50.040001  19.920000  1203.900024   \n",
      "\n",
      "      Silver  MOVE     ê´‘ì–´_KOSPI   ê´‘ì–´_USD/KRW  ...      ìš°ëŸ­_Gold  ìš°ëŸ­_Silver  \\\n",
      "0        NaN   NaN          NaN  1092.699951  ...          NaN        NaN   \n",
      "1  15.734000  70.0  1926.439941  1093.599976  ...  1186.000000  15.734000   \n",
      "2  15.734000  70.0  1926.439941  1093.599976  ...  1186.000000  15.734000   \n",
      "3  15.734000  70.0  1926.439941  1093.599976  ...  1186.000000  15.734000   \n",
      "4  16.179001  71.5  1915.750000  1111.000000  ...  1203.900024  16.179001   \n",
      "\n",
      "   ìš°ëŸ­_MOVE     ì°¸ë”_KOSPI   ì°¸ë”_USD/KRW     ì°¸ë”_WTI     ì°¸ë”_VIX      ì°¸ë”_Gold  \\\n",
      "0      NaN          NaN  1092.699951        NaN        NaN          NaN   \n",
      "1     70.0  1926.439941  1093.599976  52.689999  17.790001  1186.000000   \n",
      "2     70.0  1926.439941  1093.599976  52.689999  17.790001  1186.000000   \n",
      "3     70.0  1926.439941  1093.599976  52.689999  17.790001  1186.000000   \n",
      "4     71.5  1915.750000  1111.000000  50.040001  19.920000  1203.900024   \n",
      "\n",
      "   ì°¸ë”_Silver  ì°¸ë”_MOVE  \n",
      "0        NaN      NaN  \n",
      "1  15.734000     70.0  \n",
      "2  15.734000     70.0  \n",
      "3  15.734000     70.0  \n",
      "4  16.179001     71.5  \n",
      "\n",
      "[5 rows x 57 columns]\n",
      "\n",
      "ğŸ“ ì²˜ë¦¬ëœ íŒŒì¼ì´ 'expanded_economic_indicators.csv'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "file_path = \"filled_economic_indicators.csv\"  # ì›ë³¸ ë°ì´í„° íŒŒì¼\n",
    "output_file = \"expanded_economic_indicators.csv\"  # ì €ì¥ë  íŒŒì¼\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# ë³µì‚¬í•  ê²½ì œ ì§€í‘œ ë¦¬ìŠ¤íŠ¸\n",
    "economic_indicators = [\"KOSPI\", \"USD/KRW\", \"WTI\", \"VIX\", \"Gold\", \"Silver\", \"MOVE\"]\n",
    "\n",
    "# ìˆ˜ì‚°ë¬¼ ë¦¬ìŠ¤íŠ¸\n",
    "fish_types = [\"ê´‘ì–´\", \"ë†ì–´\", \"ëŒ€ê²Œ\", \"ë°©ì–´\", \"ì—°ì–´\", \"ìš°ëŸ­\", \"ì°¸ë”\"]\n",
    "\n",
    "# ìƒˆë¡œìš´ ì¹¼ëŸ¼ ìƒì„± (ê° ê²½ì œ ì§€í‘œë¥¼ ìˆ˜ì‚°ë¬¼ë³„ë¡œ ë³µì‚¬)\n",
    "for fish in fish_types:\n",
    "    for indicator in economic_indicators:\n",
    "        df[f\"{fish}_{indicator}\"] = df[indicator]\n",
    "\n",
    "# ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "# ë³€ê²½ëœ ë°ì´í„° í™•ì¸\n",
    "print(\"\\nâœ… ë³€ê²½ëœ ë°ì´í„° (ìƒìœ„ 5ê°œ í–‰):\")\n",
    "print(df.head())\n",
    "\n",
    "print(f\"\\nğŸ“ ì²˜ë¦¬ëœ íŒŒì¼ì´ '{output_file}'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. íŠ¸ë Œë“œ ë°ì´í„° ì–´ì¢…ë³„ë¡œ ê·¸ë£¹í™”\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-1. ê·¸ë£¹í™” í•¨ìˆ˜ ìƒì„± ë° ì €ì¥\n",
    "\n",
    "* ì…ë ¥ íŒŒì¼ : 'nst_{fish}_trend_2025-##-##.csv'\n",
    "* ì¶œë ¥ íŒŒì¼ : 'ê·¸ë£¹í™”_nst_{fish}_trend.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_age_groups(file_path, fish_type):\n",
    "    # CSV íŒŒì¼ ì½ê¸°\n",
    "    df = pd.read_csv(file_path, encoding='utf-8')\n",
    "    \n",
    "    # ì—°ë ¹ëŒ€ ê·¸ë£¹ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ìƒì„±\n",
    "    age_mapping = {\n",
    "        '19_24': '20ëŒ€',\n",
    "        '25_29': '20ëŒ€',\n",
    "        '30_34': '30ëŒ€',\n",
    "        '35_39': '30ëŒ€',\n",
    "        '40_44': '40ëŒ€',\n",
    "        '45_49': '40ëŒ€',\n",
    "        '50_54': '50ëŒ€',\n",
    "        '55_59': '50ëŒ€',\n",
    "        '60_80': '60ëŒ€ì´ìƒ'\n",
    "    }\n",
    "    \n",
    "    # ì›í•˜ëŠ” ì—°ë ¹ëŒ€ë§Œ í•„í„°ë§\n",
    "    df = df[df['age'].isin(age_mapping.keys())]\n",
    "    \n",
    "    # ìƒˆë¡œìš´ ì—°ë ¹ëŒ€ ì»¬ëŸ¼ ìƒì„±\n",
    "    df['age_group'] = df['age'].map(age_mapping)\n",
    "    \n",
    "    # ì¼ìë³„, ìƒˆë¡œìš´ ì—°ë ¹ëŒ€ë³„ë¡œ score í•©ì‚°\n",
    "    result = df.groupby(['date', 'name', 'age_group'])['score'].sum().reset_index()\n",
    "    \n",
    "    # í”¼ë²— í…Œì´ë¸”ë¡œ ë³€í™˜í•˜ì—¬ ë³´ê¸° ì¢‹ê²Œ ì •ë¦¬\n",
    "    pivot_result = result.pivot(index=['date', 'name'], \n",
    "                              columns='age_group', \n",
    "                              values='score').reset_index()\n",
    "    \n",
    "    # ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬\n",
    "    column_order = [ 'date', 'name', '20ëŒ€', '30ëŒ€', '40ëŒ€', '50ëŒ€', '60ëŒ€ì´ìƒ']\n",
    "    pivot_result = pivot_result[column_order]\n",
    "    \n",
    "    return pivot_result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 'ê·¸ë£¹í™”_nst_ê´‘ì–´_trend.csv' ì €ì¥ ì™„ë£Œ!\n",
      "âœ… 'ê·¸ë£¹í™”_nst_ë†ì–´_trend.csv' ì €ì¥ ì™„ë£Œ!\n",
      "âœ… 'ê·¸ë£¹í™”_nst_ëŒ€ê²Œ_trend.csv' ì €ì¥ ì™„ë£Œ!\n",
      "âœ… 'ê·¸ë£¹í™”_nst_ë°©ì–´_trend.csv' ì €ì¥ ì™„ë£Œ!\n",
      "âœ… 'ê·¸ë£¹í™”_nst_ì—°ì–´_trend.csv' ì €ì¥ ì™„ë£Œ!\n",
      "âœ… 'ê·¸ë£¹í™”_nst_ìš°ëŸ­_trend.csv' ì €ì¥ ì™„ë£Œ!\n",
      "âœ… 'ê·¸ë£¹í™”_nst_ì°¸ë”_trend.csv' ì €ì¥ ì™„ë£Œ!\n",
      "\n",
      "ğŸ“Š ìµœì¢… ì²˜ë¦¬ëœ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸° (ê°ê° ìƒìœ„ 3ê°œ í–‰)\n",
      "\n",
      "ê´‘ì–´ ë°ì´í„° ìƒ˜í”Œ:\n",
      "age_group        date name       20ëŒ€       30ëŒ€       40ëŒ€       50ëŒ€     60ëŒ€ì´ìƒ\n",
      "0          2016-01-01   ê´‘ì–´  3.716023  3.554474  3.326419  3.809975  1.532954\n",
      "1          2016-01-02   ê´‘ì–´  3.827210  3.382407  3.133596  4.049620  1.812825\n",
      "2          2016-01-03   ê´‘ì–´  3.682096  3.701303  3.488837  3.279367  1.677356\n",
      "--------------------------------------------------\n",
      "ë†ì–´ ë°ì´í„° ìƒ˜í”Œ:\n",
      "age_group        date name       20ëŒ€       30ëŒ€       40ëŒ€       50ëŒ€     60ëŒ€ì´ìƒ\n",
      "0          2016-01-01   ë†ì–´  3.260213  3.581407  3.558843  5.390086  2.163250\n",
      "1          2016-01-02   ë†ì–´  3.018128  3.318280  3.222967  3.078885  1.804692\n",
      "2          2016-01-03   ë†ì–´  2.932536  3.082965  2.882074  4.802194  1.134115\n",
      "--------------------------------------------------\n",
      "ëŒ€ê²Œ ë°ì´í„° ìƒ˜í”Œ:\n",
      "age_group        date name       20ëŒ€        30ëŒ€        40ëŒ€       50ëŒ€     60ëŒ€ì´ìƒ\n",
      "0          2016-01-01   ëŒ€ê²Œ  9.088761  15.402608  12.426487  9.709127  3.046450\n",
      "1          2016-01-02   ëŒ€ê²Œ  7.991422  12.912363   9.378193  7.434733  3.600261\n",
      "2          2016-01-03   ëŒ€ê²Œ  7.005608   9.586618   7.596756  5.591602  2.300119\n",
      "--------------------------------------------------\n",
      "ë°©ì–´ ë°ì´í„° ìƒ˜í”Œ:\n",
      "age_group        date name       20ëŒ€       30ëŒ€        40ëŒ€        50ëŒ€     60ëŒ€ì´ìƒ\n",
      "0          2016-01-01   ë°©ì–´  5.906486  7.146976   6.614764  14.358005  3.340613\n",
      "1          2016-01-02   ë°©ì–´  6.485972  9.341901  10.622029  17.453441  7.421370\n",
      "2          2016-01-03   ë°©ì–´  5.147320  5.948592   6.213011   6.207540  2.696208\n",
      "--------------------------------------------------\n",
      "ì—°ì–´ ë°ì´í„° ìƒ˜í”Œ:\n",
      "age_group        date name        20ëŒ€        30ëŒ€        40ëŒ€        50ëŒ€  \\\n",
      "0          2016-01-01   ì—°ì–´   4.809541   4.894878   4.750223   5.961751   \n",
      "1          2016-01-02   ì—°ì–´   6.986391   6.831603   5.381617   5.615416   \n",
      "2          2016-01-03   ì—°ì–´  12.569106  16.147838  12.739428  13.177470   \n",
      "\n",
      "age_group     60ëŒ€ì´ìƒ  \n",
      "0          1.338870  \n",
      "1          3.040053  \n",
      "2          5.182101  \n",
      "--------------------------------------------------\n",
      "ìš°ëŸ­ ë°ì´í„° ìƒ˜í”Œ:\n",
      "age_group        date name       20ëŒ€       30ëŒ€       40ëŒ€       50ëŒ€     60ëŒ€ì´ìƒ\n",
      "0          2016-01-01   ìš°ëŸ­  3.628428  3.449493  3.129530  4.836146  1.323523\n",
      "1          2016-01-02   ìš°ëŸ­  3.745991  3.395083  2.909397  4.233117  1.549985\n",
      "2          2016-01-03   ìš°ëŸ­  3.453351  3.871255  3.303071  3.276261  1.962474\n",
      "--------------------------------------------------\n",
      "ì°¸ë” ë°ì´í„° ìƒ˜í”Œ:\n",
      "age_group        date name       20ëŒ€       30ëŒ€       40ëŒ€       50ëŒ€     60ëŒ€ì´ìƒ\n",
      "0          2016-01-01   ì°¸ë”  3.181044  3.557813  3.030270  3.608375  1.308016\n",
      "1          2016-01-02   ì°¸ë”  3.371687  3.698529  3.199715  3.968064  1.387602\n",
      "2          2016-01-03   ì°¸ë”  3.387131  3.544599  3.112130  3.432890  1.387602\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#  ìˆ˜ì‚°ë¬¼ ì¢…ë¥˜ë³„ íŒŒì¼ ê²½ë¡œ ë° ì´ë¦„ ì„¤ì •\n",
    "fish_files = {\n",
    "    'ê´‘ì–´': 'nst_ê´‘ì–´_trend_2025-01-30.csv',\n",
    "    'ë†ì–´': 'nst_ë†ì–´_trend_2025-01-30.csv',\n",
    "    'ëŒ€ê²Œ': 'nst_ëŒ€ê²Œ_trend_2025-01-30.csv',\n",
    "    'ë°©ì–´': 'nst_ë°©ì–´_trend_2025-01-30.csv',\n",
    "    'ì—°ì–´': 'nst_ì—°ì–´_trend_2025-01-30.csv',\n",
    "    'ìš°ëŸ­': 'nst_ìš°ëŸ­_trend_2025-01-30.csv',\n",
    "    'ì°¸ë”': 'nst_ì°¸ë”_trend_2025-01-30.csv'\n",
    "}\n",
    "\n",
    "#  ê°œë³„ íŒŒì¼ ì €ì¥ + ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì¶œë ¥ ì¤€ë¹„\n",
    "all_results = {}\n",
    "\n",
    "#  ëª¨ë“  íŒŒì¼ ì²˜ë¦¬\n",
    "for fish, file_path in fish_files.items():\n",
    "    try:\n",
    "        processed_df = process_age_groups(file_path, fish)\n",
    "\n",
    "        # ê°œë³„ íŒŒì¼ ì €ì¥\n",
    "        output_filename = f'ê·¸ë£¹í™”_nst_{fish}_trend.csv'\n",
    "        processed_df.to_csv(output_filename, index=False)\n",
    "\n",
    "        # ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ (ì²« 3ê°œ í–‰ë§Œ ì €ì¥)\n",
    "        all_results[fish] = processed_df.head(3)\n",
    "\n",
    "        print(f\"âœ… '{output_filename}' ì €ì¥ ì™„ë£Œ!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing {fish}: {e}\")\n",
    "\n",
    "#  ìµœì¢… ì²˜ë¦¬ëœ ê²°ê³¼ ì¶œë ¥ (ê°ê° 3ê°œ í–‰ë§Œ ì¶œë ¥)\n",
    "print(\"\\nğŸ“Š ìµœì¢… ì²˜ë¦¬ëœ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸° (ê°ê° ìƒìœ„ 3ê°œ í–‰)\\n\")\n",
    "for fish, df_sample in all_results.items():\n",
    "    print(f\"{fish} ë°ì´í„° ìƒ˜í”Œ:\")\n",
    "    print(df_sample)\n",
    "    print(\"-\" * 50)  # êµ¬ë¶„ì„  ì¶”ê°€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-2. merged_trends.csv ì €ì¥\n",
    "\n",
    "* ì…ë ¥ íŒŒì¼ : 'ê·¸ë£¹í™”_nst_{fish}_trend.csv'\n",
    "* ì¶œë ¥ íŒŒì¼ : 'merged_trends.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def merge_fish_trends(file_paths, output_path):\n",
    "    \"\"\"\n",
    "    ì—¬ëŸ¬ ê°œì˜ ìˆ˜ì‚°ë¬¼ íŠ¸ë Œë“œ ë°ì´í„°ë¥¼ ë³‘í•©í•˜ì—¬ ë‚ ì§œë³„ë¡œ ì •ë¦¬í•˜ëŠ” í•¨ìˆ˜.\n",
    "    \n",
    "    Args:\n",
    "        file_paths (list): CSV íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸.\n",
    "        output_path (str): ë³‘í•©ëœ ë°ì´í„°ë¥¼ ì €ì¥í•  íŒŒì¼ ê²½ë¡œ.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: ë³‘í•©ëœ ë°ì´í„°í”„ë ˆì„.\n",
    "    \"\"\"\n",
    "    # ì²« ë²ˆì§¸ íŒŒì¼ ë¡œë“œ ë° 'name' ì»¬ëŸ¼ ì œê±°\n",
    "    merged_df = pd.read_csv(file_paths[0], encoding='utf-8')\n",
    "    merged_df = merged_df.drop(columns=['name'], errors='ignore')  # name ì»¬ëŸ¼ ì‚­ì œ (ì—†ì–´ë„ ì˜¤ë¥˜ ë°©ì§€)\n",
    "    merged_df['date'] = pd.to_datetime(merged_df['date'])  # ë‚ ì§œ í˜•ì‹ ë³€í™˜\n",
    "    \n",
    "    # íŒŒì¼ëª…ì—ì„œ ìˆ˜ì‚°ë¬¼ ì´ë¦„ ì •í™•íˆ ì¶”ì¶œ\n",
    "    fish_name = file_paths[0].split('_')[2]  # íŒŒì¼ëª… ì˜ˆ: 'ê·¸ë£¹í™”_nst_ê´‘ì–´_trend_2025-01-17.csv' â†’ 'ê´‘ì–´'\n",
    "    \n",
    "    # ì»¬ëŸ¼ëª… ë³€ê²½ (ì˜ˆ: '20ëŒ€' â†’ 'ê´‘ì–´_20ëŒ€')\n",
    "    merged_df = merged_df.rename(columns={col: f\"{fish_name}_{col}\" for col in merged_df.columns if col not in ['date']})\n",
    "\n",
    "    # ë‚˜ë¨¸ì§€ íŒŒì¼ë“¤ ë³‘í•©\n",
    "    for file_path in file_paths[1:]:\n",
    "        df = pd.read_csv(file_path, encoding='utf-8')\n",
    "        df = df.drop(columns=['name'], errors='ignore')  # 'name' ì»¬ëŸ¼ ì‚­ì œ\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "        # íŒŒì¼ëª…ì—ì„œ ìˆ˜ì‚°ë¬¼ ì´ë¦„ ì •í™•íˆ ì¶”ì¶œ\n",
    "        fish_name = file_path.split('_')[2]  # 'ê·¸ë£¹í™”_nst_ë†ì–´_trend_2025-01-17.csv' â†’ 'ë†ì–´'\n",
    "        \n",
    "        # ì»¬ëŸ¼ëª… ë³€ê²½ (ì˜ˆ: '20ëŒ€' â†’ 'ë†ì–´_20ëŒ€')\n",
    "        df = df.rename(columns={col: f\"{fish_name}_{col}\" for col in df.columns if col not in ['date']})\n",
    "\n",
    "        # ë³‘í•© (ì¤‘ë³µ ì»¬ëŸ¼ ì œê±°)\n",
    "        merged_df = pd.merge(merged_df, df, on='date', how='outer', suffixes=(None, None))\n",
    "\n",
    "        # ì¤‘ë³µ ì»¬ëŸ¼ ìë™ ì œê±° (e.g. 'ê´‘ì–´_20ëŒ€_x', 'ê´‘ì–´_20ëŒ€_y')\n",
    "        merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]\n",
    "\n",
    "    # ë‚ ì§œìˆœ ì •ë ¬\n",
    "    result = merged_df.sort_values('date')\n",
    "\n",
    "    # CSV ì €ì¥\n",
    "    result.to_csv(output_path, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"\\nâœ… ë³‘í•©ëœ ë°ì´í„°ê°€ '{output_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… ë³‘í•©ëœ ë°ì´í„°ê°€ 'merged_trends.csv'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\n",
      "\n",
      "âœ… ë³‘í•©ëœ ë°ì´í„° (ìƒìœ„ 5ê°œ í–‰):\n",
      "        date    ê´‘ì–´_20ëŒ€    ê´‘ì–´_30ëŒ€    ê´‘ì–´_40ëŒ€    ê´‘ì–´_50ëŒ€  ê´‘ì–´_60ëŒ€ì´ìƒ    ë†ì–´_20ëŒ€  \\\n",
      "0 2016-01-01  3.716023  3.554474  3.326419  3.809975  1.532954  3.260213   \n",
      "1 2016-01-02  3.827210  3.382407  3.133596  4.049620  1.812825  3.018128   \n",
      "2 2016-01-03  3.682096  3.701303  3.488837  3.279367  1.677356  2.932536   \n",
      "3 2016-01-04  3.449136  3.227122  2.811551  3.672470  1.940197  2.730419   \n",
      "4 2016-01-05  3.401118  3.322712  3.168678  2.483900  1.000000  2.676719   \n",
      "\n",
      "     ë†ì–´_30ëŒ€    ë†ì–´_40ëŒ€    ë†ì–´_50ëŒ€  ...    ìš°ëŸ­_20ëŒ€    ìš°ëŸ­_30ëŒ€    ìš°ëŸ­_40ëŒ€    ìš°ëŸ­_50ëŒ€  \\\n",
      "0  3.581407  3.558843  5.390086  ...  3.628428  3.449493  3.129530  4.836146   \n",
      "1  3.318280  3.222967  3.078885  ...  3.745991  3.395083  2.909397  4.233117   \n",
      "2  3.082965  2.882074  4.802194  ...  3.453351  3.871255  3.303071  3.276261   \n",
      "3  3.361940  3.029135  3.059708  ...  3.333151  3.069274  3.539530  4.207426   \n",
      "4  3.040365  3.129464  3.153394  ...  3.304805  3.384335  2.707205  2.638036   \n",
      "\n",
      "   ìš°ëŸ­_60ëŒ€ì´ìƒ    ì°¸ë”_20ëŒ€    ì°¸ë”_30ëŒ€    ì°¸ë”_40ëŒ€    ì°¸ë”_50ëŒ€  ì°¸ë”_60ëŒ€ì´ìƒ  \n",
      "0  1.323523  3.181044  3.557813  3.030270  3.608375  1.308016  \n",
      "1  1.549985  3.371687  3.698529  3.199715  3.968064  1.387602  \n",
      "2  1.962474  3.387131  3.544599  3.112130  3.432890  1.387602  \n",
      "3  1.637661  2.941569  2.955552  2.781371  3.147489  1.296743  \n",
      "4  1.000000  2.821804  3.201582  2.908575  2.748799  1.279332  \n",
      "\n",
      "[5 rows x 36 columns]\n"
     ]
    }
   ],
   "source": [
    "# ì‚¬ìš© ì˜ˆì‹œ:\n",
    "files = [\n",
    "    'ê·¸ë£¹í™”_nst_ê´‘ì–´_trend.csv', \n",
    "    'ê·¸ë£¹í™”_nst_ë†ì–´_trend.csv', \n",
    "    'ê·¸ë£¹í™”_nst_ëŒ€ê²Œ_trend.csv', \n",
    "    'ê·¸ë£¹í™”_nst_ë°©ì–´_trend.csv', \n",
    "    'ê·¸ë£¹í™”_nst_ì—°ì–´_trend.csv', \n",
    "    'ê·¸ë£¹í™”_nst_ìš°ëŸ­_trend.csv', \n",
    "    'ê·¸ë£¹í™”_nst_ì°¸ë”_trend.csv'\n",
    "]\n",
    "\n",
    "# ë³‘í•© ì‹¤í–‰\n",
    "merged_result = merge_fish_trends(files, 'merged_trends.csv')\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸\n",
    "print(\"\\nâœ… ë³‘í•©ëœ ë°ì´í„° (ìƒìœ„ 5ê°œ í–‰):\")\n",
    "print(merged_result.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ê¸°ìƒ ë°ì´í„° ì „ì²˜ë¦¬\n",
    "\n",
    "* ì…ë ¥ íŒŒì¼ : 'forecast_agg.csv'\n",
    "* ì¶œë ¥ íŒŒì¼ : 'weatherdata_processed.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•¨ìˆ˜ ì •ì˜\n",
    "\n",
    "def create_weather_columns(df, station_cols, output_path):\n",
    "   result_df = df.copy()\n",
    "   result_df['ì¼ì‹œ'] = pd.to_datetime(result_df['ì¼ì‹œ'])\n",
    "   \n",
    "   final_df = pd.DataFrame({'ì¼ì‹œ': result_df['ì¼ì‹œ'].unique()})\n",
    "   \n",
    "   for station, columns in station_cols.items():\n",
    "       station_data = result_df[result_df['ì§€ì '] == station].copy()\n",
    "       \n",
    "       for orig_col, new_col in columns:\n",
    "           station_col = station_data[['ì¼ì‹œ', orig_col]].copy()\n",
    "           final_df = pd.merge(final_df, station_col.rename(columns={orig_col: new_col}), \n",
    "                             on='ì¼ì‹œ', how='left')\n",
    "   \n",
    "   # ì¹¼ëŸ¼ì„ ê°€ë‚˜ë‹¤ìˆœìœ¼ë¡œ ì •ë ¬ ('ì¼ì‹œ' ì»¬ëŸ¼ì€ ì²«ë²ˆì§¸ë¡œ ìœ ì§€)\n",
    "   sorted_cols = ['ì¼ì‹œ'] + sorted([col for col in final_df.columns if col != 'ì¼ì‹œ'])\n",
    "   result = final_df[sorted_cols].sort_values('ì¼ì‹œ')\n",
    "   \n",
    "   result.to_csv(output_path, index=False, encoding='utf-8')\n",
    "   return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV íŒŒì¼ ì½ê¸°\n",
    "df = pd.read_csv('forecast_agg.csv', encoding='utf-8')\n",
    "\n",
    "# í”¼ì³ ì„ íƒ\n",
    "station_columns = {\n",
    "\t22105: [\n",
    "\t\t('ê¸°ì˜¨', 'ê´‘ì–´_ê¸°ì˜¨_22105'),\n",
    "\t\t('ê¸°ì˜¨', 'ë†ì–´_ê¸°ì˜¨_22105'),\n",
    "\t\t('ê¸°ì˜¨', 'ëŒ€ê²Œ_ê¸°ì˜¨_22105'),\n",
    "\t\t('íŒŒì£¼ê¸°', 'ëŒ€ê²Œ_íŒŒì£¼ê¸°_22105'),\n",
    "\t\t('íŒŒì£¼ê¸°', 'ë°©ì–´_íŒŒì£¼ê¸°_22105'),\n",
    "\t\t('ê¸°ì˜¨', 'ì—°ì–´_ê¸°ì˜¨_22105'),\n",
    "\t\t('ìŠµë„', 'ì—°ì–´_ìŠµë„_22105')\n",
    "\t],\n",
    "    \n",
    "\t22107: [\n",
    "\t\t('ìˆ˜ì˜¨', 'ê´‘ì–´_ìˆ˜ì˜¨_22107'),\n",
    "\t\t('ìˆ˜ì˜¨', 'ë†ì–´_ìˆ˜ì˜¨_22107'),\n",
    "\t\t('ìˆ˜ì˜¨', 'ë°©ì–´_ìˆ˜ì˜¨_22107'),\n",
    "\t\t('ìˆ˜ì˜¨', 'ì—°ì–´_ìˆ˜ì˜¨_22107'),\n",
    "\t\t('ìˆ˜ì˜¨', 'ì°¸ë”_ìˆ˜ì˜¨_22107')\n",
    "\t],\n",
    "\n",
    "\t22186: [\n",
    "\t\t('ìŠµë„', 'ê´‘ì–´_ìŠµë„_22186'),\n",
    "\t\t('ìŠµë„', 'ë†ì–´_ìŠµë„_22186'),\n",
    "\t\t('ê¸°ì˜¨', 'ìš°ëŸ­_ê¸°ì˜¨_22186'),\n",
    "\t\t('ìˆ˜ì˜¨', 'ìš°ëŸ­_ìˆ˜ì˜¨_22186')\n",
    "\t\t],\n",
    "        \n",
    "\t22188: [\n",
    "\t\t('ìˆ˜ì˜¨', 'ëŒ€ê²Œ_ìˆ˜ì˜¨_22188'),\n",
    "\t\t('ìŠµë„', 'ëŒ€ê²Œ_ìŠµë„_22188')\n",
    "\t\t],        \n",
    "\n",
    "\t22189: [\n",
    "\t\t('íŒŒì£¼ê¸°', 'ìš°ëŸ­_íŒŒì£¼ê¸°_22189')\n",
    "\t\t],       \n",
    "\n",
    "\t22190: [\n",
    "\t\t('íŒŒì£¼ê¸°', 'ê´‘ì–´_íŒŒì£¼ê¸°_22190'),\n",
    "        ('íŒŒì£¼ê¸°', 'ë†ì–´_íŒŒì£¼ê¸°_22190'),\n",
    "        ('ê¸°ì˜¨', 'ë°©ì–´_ê¸°ì˜¨_22190'),\n",
    "        ('ìŠµë„', 'ë°©ì–´_ìŠµë„_22190'),\n",
    "        ('íŒŒì£¼ê¸°', 'ì—°ì–´_íŒŒì£¼ê¸°_22190'),\n",
    "        ('ìŠµë„', 'ìš°ëŸ­_ìŠµë„_22190'),\n",
    "        ('ê¸°ì˜¨', 'ì°¸ë”_ê¸°ì˜¨_22190'),\n",
    "        ('ìŠµë„', 'ì°¸ë”_ìŠµë„_22190'),\n",
    "        ('íŒŒì£¼ê¸°', 'ì°¸ë”_íŒŒì£¼ê¸°_22190')\n",
    "\t\t]  \n",
    "\n",
    "\t}\n",
    "\n",
    "result = create_weather_columns(df, station_columns, 'weatherdata_processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. ë³€ìˆ˜ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°\n",
    "\n",
    "* ì…ë ¥ íŒŒì¼ : 'expanded_economic_indicators_.csv',  'merged_trends.csv', 'weatherdata_processed.csv'\n",
    "* ì¶œë ¥ íŒŒì¼ : 'merged_all_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_data(output_path='merged_all_data.csv'):\n",
    "   # ê° íŒŒì¼ ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "   \n",
    "   economic = pd.read_csv('expanded_economic_indicators.csv', encoding='utf-8')\n",
    "   economic['Date'] = pd.to_datetime(economic['Date'])\n",
    "   economic = economic.rename(columns={'Date': 'ë‚ ì§œ'})\n",
    "   \n",
    "   trends = pd.read_csv('merged_trends.csv', encoding='utf-8')\n",
    "   trends['date'] = pd.to_datetime(trends['date'])\n",
    "   trends = trends.rename(columns={'date': 'ë‚ ì§œ'})\n",
    "   \n",
    "   weather = pd.read_csv('weatherdata_processed.csv', encoding='utf-8')\n",
    "   weather['ì¼ì‹œ'] = pd.to_datetime(weather['ì¼ì‹œ'])\n",
    "   weather = weather.rename(columns={'ì¼ì‹œ': 'ë‚ ì§œ'})\n",
    "   \n",
    "   # ëª¨ë“  ë‚ ì§œ ì¶”ì¶œ\n",
    "   all_dates = pd.concat([\n",
    "        economic['ë‚ ì§œ'], \n",
    "        trends['ë‚ ì§œ'], \n",
    "       weather['ë‚ ì§œ']\n",
    "   ]).unique()\n",
    "   \n",
    "   # ë‚ ì§œ ê¸°ì¤€ ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "   date_df = pd.DataFrame({'ë‚ ì§œ': all_dates})\n",
    "   date_df = date_df.sort_values('ë‚ ì§œ')\n",
    "   \n",
    "   # ë°ì´í„° ë³‘í•©\n",
    "   dfs = [\n",
    "\t\tdate_df, \n",
    "\t\teconomic, \n",
    "\t\ttrends,\n",
    "\t\tweather\n",
    "   ]\n",
    "   \n",
    "   result = dfs[0]\n",
    "   for df in dfs[1:]:\n",
    "       result = pd.merge(result, df, on='ë‚ ì§œ', how='left')\n",
    "   \n",
    "   # ê²°ê³¼ ì €ì¥\n",
    "   result.to_csv(output_path, index=False, encoding='utf-8')\n",
    "   return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = merge_all_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°\n",
    "\n",
    "* ì…ë ¥ íŒŒì¼ : 'merged_all_data.csv'\n",
    "* ì¶œë ¥ íŒŒì¼ : 'filled_merged_all_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def fill_missing_values(file_path, output_path):\n",
    "    \"\"\"\n",
    "    CSV íŒŒì¼ì—ì„œ ê²°ì¸¡ì¹˜ë¥¼ `ffill`ì„ ì‚¬ìš©í•˜ì—¬ ì±„ìš°ë˜, ì´ì „ ê°’ì´ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë‘ .\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): ì…ë ¥ CSV íŒŒì¼ ê²½ë¡œ.\n",
    "        output_path (str): ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ì €ì¥í•  ê²½ë¡œ.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: ê²°ì¸¡ì¹˜ê°€ ì±„ì›Œì§„ ë°ì´í„°í”„ë ˆì„.\n",
    "    \"\"\"\n",
    "    \n",
    "    # CSV íŒŒì¼ ë¡œë“œ\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # ê²°ì¸¡ì¹˜ í™•ì¸ (ì²˜ë¦¬ ì „)\n",
    "    missing_before = df.isnull().sum()\n",
    "\n",
    "    # ê²°ì¸¡ì¹˜ `ffill`ë¡œ ì±„ìš°ê¸° (ì´ì „ ê°’ì´ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë‘ )\n",
    "    df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "    # ê²°ì¸¡ì¹˜ í™•ì¸ (ì²˜ë¦¬ í›„)\n",
    "    missing_after = df.isnull().sum()\n",
    "\n",
    "    # ë³€ê²½ëœ ê²°ì¸¡ì¹˜ ê°œìˆ˜ ì¶œë ¥\n",
    "    missing_summary = pd.DataFrame({'Before': missing_before, 'After': missing_after})\n",
    "    print(\"\\nğŸ” ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì „í›„ ë¹„êµ:\")\n",
    "    print(missing_summary)\n",
    "\n",
    "    # ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"\\nâœ… ì²˜ë¦¬ëœ ë°ì´í„°ê°€ '{output_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_39148\\1915651589.py:22: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì „í›„ ë¹„êµ:\n",
      "              Before  After\n",
      "ë‚ ì§œ                 0      0\n",
      "USD/KRW            0      0\n",
      "KOSPI              1      1\n",
      "WTI                1      1\n",
      "VIX                1      1\n",
      "...              ...    ...\n",
      "ìš°ëŸ­_íŒŒì£¼ê¸°_22189     494    355\n",
      "ì°¸ë”_ê¸°ì˜¨_22190      459    342\n",
      "ì°¸ë”_ìˆ˜ì˜¨_22107      210      0\n",
      "ì°¸ë”_ìŠµë„_22190      461    342\n",
      "ì°¸ë”_íŒŒì£¼ê¸°_22190     421    342\n",
      "\n",
      "[120 rows x 2 columns]\n",
      "\n",
      "âœ… ì²˜ë¦¬ëœ ë°ì´í„°ê°€ 'filled_merged_all_data.csv'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ë‚ ì§œ</th>\n",
       "      <th>USD/KRW</th>\n",
       "      <th>KOSPI</th>\n",
       "      <th>WTI</th>\n",
       "      <th>VIX</th>\n",
       "      <th>Gold</th>\n",
       "      <th>Silver</th>\n",
       "      <th>MOVE</th>\n",
       "      <th>ê´‘ì–´_KOSPI</th>\n",
       "      <th>ê´‘ì–´_USD/KRW</th>\n",
       "      <th>...</th>\n",
       "      <th>ì—°ì–´_ìŠµë„_22105</th>\n",
       "      <th>ì—°ì–´_íŒŒì£¼ê¸°_22190</th>\n",
       "      <th>ìš°ëŸ­_ê¸°ì˜¨_22186</th>\n",
       "      <th>ìš°ëŸ­_ìˆ˜ì˜¨_22186</th>\n",
       "      <th>ìš°ëŸ­_ìŠµë„_22190</th>\n",
       "      <th>ìš°ëŸ­_íŒŒì£¼ê¸°_22189</th>\n",
       "      <th>ì°¸ë”_ê¸°ì˜¨_22190</th>\n",
       "      <th>ì°¸ë”_ìˆ˜ì˜¨_22107</th>\n",
       "      <th>ì°¸ë”_ìŠµë„_22190</th>\n",
       "      <th>ì°¸ë”_íŒŒì£¼ê¸°_22190</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>1092.699951</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1092.699951</td>\n",
       "      <td>...</td>\n",
       "      <td>65.130435</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.382609</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>1093.599976</td>\n",
       "      <td>1926.439941</td>\n",
       "      <td>52.689999</td>\n",
       "      <td>17.790001</td>\n",
       "      <td>1186.000000</td>\n",
       "      <td>15.734000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>1926.439941</td>\n",
       "      <td>1093.599976</td>\n",
       "      <td>...</td>\n",
       "      <td>60.083333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.375000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-03</td>\n",
       "      <td>1093.599976</td>\n",
       "      <td>1926.439941</td>\n",
       "      <td>52.689999</td>\n",
       "      <td>17.790001</td>\n",
       "      <td>1186.000000</td>\n",
       "      <td>15.734000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>1926.439941</td>\n",
       "      <td>1093.599976</td>\n",
       "      <td>...</td>\n",
       "      <td>39.750000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.283333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1093.599976</td>\n",
       "      <td>1926.439941</td>\n",
       "      <td>52.689999</td>\n",
       "      <td>17.790001</td>\n",
       "      <td>1186.000000</td>\n",
       "      <td>15.734000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>1926.439941</td>\n",
       "      <td>1093.599976</td>\n",
       "      <td>...</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.475000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-05</td>\n",
       "      <td>1111.000000</td>\n",
       "      <td>1915.750000</td>\n",
       "      <td>50.040001</td>\n",
       "      <td>19.920000</td>\n",
       "      <td>1203.900024</td>\n",
       "      <td>16.179001</td>\n",
       "      <td>71.500000</td>\n",
       "      <td>1915.750000</td>\n",
       "      <td>1111.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>57.086957</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.475000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3677</th>\n",
       "      <td>2025-01-25</td>\n",
       "      <td>1433.930054</td>\n",
       "      <td>2536.800049</td>\n",
       "      <td>74.660004</td>\n",
       "      <td>14.850000</td>\n",
       "      <td>2777.300049</td>\n",
       "      <td>31.023001</td>\n",
       "      <td>86.750000</td>\n",
       "      <td>2536.800049</td>\n",
       "      <td>1433.930054</td>\n",
       "      <td>...</td>\n",
       "      <td>53.545455</td>\n",
       "      <td>6.004167</td>\n",
       "      <td>3.2375</td>\n",
       "      <td>9.683333</td>\n",
       "      <td>59.458333</td>\n",
       "      <td>5.3375</td>\n",
       "      <td>5.825</td>\n",
       "      <td>16.263636</td>\n",
       "      <td>59.458333</td>\n",
       "      <td>6.004167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3678</th>\n",
       "      <td>2025-01-26</td>\n",
       "      <td>1433.930054</td>\n",
       "      <td>2536.800049</td>\n",
       "      <td>74.660004</td>\n",
       "      <td>14.850000</td>\n",
       "      <td>2777.300049</td>\n",
       "      <td>31.023001</td>\n",
       "      <td>86.750000</td>\n",
       "      <td>2536.800049</td>\n",
       "      <td>1433.930054</td>\n",
       "      <td>...</td>\n",
       "      <td>53.545455</td>\n",
       "      <td>6.004167</td>\n",
       "      <td>3.2375</td>\n",
       "      <td>9.683333</td>\n",
       "      <td>59.458333</td>\n",
       "      <td>5.3375</td>\n",
       "      <td>5.825</td>\n",
       "      <td>16.263636</td>\n",
       "      <td>59.458333</td>\n",
       "      <td>6.004167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3679</th>\n",
       "      <td>2025-01-27</td>\n",
       "      <td>1427.630005</td>\n",
       "      <td>2536.800049</td>\n",
       "      <td>73.169998</td>\n",
       "      <td>17.900000</td>\n",
       "      <td>2737.500000</td>\n",
       "      <td>30.254000</td>\n",
       "      <td>95.050003</td>\n",
       "      <td>2536.800049</td>\n",
       "      <td>1427.630005</td>\n",
       "      <td>...</td>\n",
       "      <td>53.545455</td>\n",
       "      <td>6.004167</td>\n",
       "      <td>3.2375</td>\n",
       "      <td>9.683333</td>\n",
       "      <td>59.458333</td>\n",
       "      <td>5.3375</td>\n",
       "      <td>5.825</td>\n",
       "      <td>16.263636</td>\n",
       "      <td>59.458333</td>\n",
       "      <td>6.004167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3680</th>\n",
       "      <td>2025-01-28</td>\n",
       "      <td>1417.920044</td>\n",
       "      <td>2536.800049</td>\n",
       "      <td>73.769997</td>\n",
       "      <td>16.410000</td>\n",
       "      <td>2766.800049</td>\n",
       "      <td>30.726999</td>\n",
       "      <td>92.830002</td>\n",
       "      <td>2536.800049</td>\n",
       "      <td>1417.920044</td>\n",
       "      <td>...</td>\n",
       "      <td>53.545455</td>\n",
       "      <td>6.004167</td>\n",
       "      <td>3.2375</td>\n",
       "      <td>9.683333</td>\n",
       "      <td>59.458333</td>\n",
       "      <td>5.3375</td>\n",
       "      <td>5.825</td>\n",
       "      <td>16.263636</td>\n",
       "      <td>59.458333</td>\n",
       "      <td>6.004167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3681</th>\n",
       "      <td>2025-01-29</td>\n",
       "      <td>1443.260010</td>\n",
       "      <td>2536.800049</td>\n",
       "      <td>72.620003</td>\n",
       "      <td>16.559999</td>\n",
       "      <td>2769.100098</td>\n",
       "      <td>31.238001</td>\n",
       "      <td>90.730003</td>\n",
       "      <td>2536.800049</td>\n",
       "      <td>1443.260010</td>\n",
       "      <td>...</td>\n",
       "      <td>53.545455</td>\n",
       "      <td>6.004167</td>\n",
       "      <td>3.2375</td>\n",
       "      <td>9.683333</td>\n",
       "      <td>59.458333</td>\n",
       "      <td>5.3375</td>\n",
       "      <td>5.825</td>\n",
       "      <td>16.263636</td>\n",
       "      <td>59.458333</td>\n",
       "      <td>6.004167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3682 rows Ã— 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ë‚ ì§œ      USD/KRW        KOSPI        WTI        VIX         Gold  \\\n",
       "0     2015-01-01  1092.699951          NaN        NaN        NaN          NaN   \n",
       "1     2015-01-02  1093.599976  1926.439941  52.689999  17.790001  1186.000000   \n",
       "2     2015-01-03  1093.599976  1926.439941  52.689999  17.790001  1186.000000   \n",
       "3     2015-01-04  1093.599976  1926.439941  52.689999  17.790001  1186.000000   \n",
       "4     2015-01-05  1111.000000  1915.750000  50.040001  19.920000  1203.900024   \n",
       "...          ...          ...          ...        ...        ...          ...   \n",
       "3677  2025-01-25  1433.930054  2536.800049  74.660004  14.850000  2777.300049   \n",
       "3678  2025-01-26  1433.930054  2536.800049  74.660004  14.850000  2777.300049   \n",
       "3679  2025-01-27  1427.630005  2536.800049  73.169998  17.900000  2737.500000   \n",
       "3680  2025-01-28  1417.920044  2536.800049  73.769997  16.410000  2766.800049   \n",
       "3681  2025-01-29  1443.260010  2536.800049  72.620003  16.559999  2769.100098   \n",
       "\n",
       "         Silver       MOVE     ê´‘ì–´_KOSPI   ê´‘ì–´_USD/KRW  ...  ì—°ì–´_ìŠµë„_22105  \\\n",
       "0           NaN        NaN          NaN  1092.699951  ...    65.130435   \n",
       "1     15.734000  70.000000  1926.439941  1093.599976  ...    60.083333   \n",
       "2     15.734000  70.000000  1926.439941  1093.599976  ...    39.750000   \n",
       "3     15.734000  70.000000  1926.439941  1093.599976  ...    54.000000   \n",
       "4     16.179001  71.500000  1915.750000  1111.000000  ...    57.086957   \n",
       "...         ...        ...          ...          ...  ...          ...   \n",
       "3677  31.023001  86.750000  2536.800049  1433.930054  ...    53.545455   \n",
       "3678  31.023001  86.750000  2536.800049  1433.930054  ...    53.545455   \n",
       "3679  30.254000  95.050003  2536.800049  1427.630005  ...    53.545455   \n",
       "3680  30.726999  92.830002  2536.800049  1417.920044  ...    53.545455   \n",
       "3681  31.238001  90.730003  2536.800049  1443.260010  ...    53.545455   \n",
       "\n",
       "      ì—°ì–´_íŒŒì£¼ê¸°_22190  ìš°ëŸ­_ê¸°ì˜¨_22186  ìš°ëŸ­_ìˆ˜ì˜¨_22186  ìš°ëŸ­_ìŠµë„_22190  ìš°ëŸ­_íŒŒì£¼ê¸°_22189  \\\n",
       "0              NaN          NaN          NaN          NaN           NaN   \n",
       "1              NaN          NaN          NaN          NaN           NaN   \n",
       "2              NaN          NaN          NaN          NaN           NaN   \n",
       "3              NaN          NaN          NaN          NaN           NaN   \n",
       "4              NaN          NaN          NaN          NaN           NaN   \n",
       "...            ...          ...          ...          ...           ...   \n",
       "3677      6.004167       3.2375     9.683333    59.458333        5.3375   \n",
       "3678      6.004167       3.2375     9.683333    59.458333        5.3375   \n",
       "3679      6.004167       3.2375     9.683333    59.458333        5.3375   \n",
       "3680      6.004167       3.2375     9.683333    59.458333        5.3375   \n",
       "3681      6.004167       3.2375     9.683333    59.458333        5.3375   \n",
       "\n",
       "      ì°¸ë”_ê¸°ì˜¨_22190  ì°¸ë”_ìˆ˜ì˜¨_22107  ì°¸ë”_ìŠµë„_22190  ì°¸ë”_íŒŒì£¼ê¸°_22190  \n",
       "0             NaN    16.382609          NaN           NaN  \n",
       "1             NaN    16.375000          NaN           NaN  \n",
       "2             NaN    16.283333          NaN           NaN  \n",
       "3             NaN    16.475000          NaN           NaN  \n",
       "4             NaN    16.475000          NaN           NaN  \n",
       "...           ...          ...          ...           ...  \n",
       "3677        5.825    16.263636    59.458333      6.004167  \n",
       "3678        5.825    16.263636    59.458333      6.004167  \n",
       "3679        5.825    16.263636    59.458333      6.004167  \n",
       "3680        5.825    16.263636    59.458333      6.004167  \n",
       "3681        5.825    16.263636    59.458333      6.004167  \n",
       "\n",
       "[3682 rows x 120 columns]"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'merged_all_data.csv'\n",
    "output_path = 'filled_merged_all_data.csv'\n",
    "\n",
    "fill_missing_values(file_path, output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. íƒ€ì„ë˜ê·¸ ì ìš©í•˜ê¸° \n",
    "\n",
    "* ì…ë ¥ íŒŒì¼ : 'filled_merged_all_data.csv'\n",
    "* ì¶œë ¥ íŒŒì¼ : 'timelagged_features.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6-1. ì„¤ì •ëœ íƒ€ì„ë˜ê·¸ ì ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_39148\\1547269827.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_{lag}'] = df[col].shift(lag)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_39148\\1547269827.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_{lag}'] = df[col].shift(lag)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_39148\\1547269827.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_{lag}'] = df[col].shift(lag)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_39148\\1547269827.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_{lag}'] = df[col].shift(lag)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_39148\\1547269827.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_{lag}'] = df[col].shift(lag)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_39148\\1547269827.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_{lag}'] = df[col].shift(lag)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_39148\\1547269827.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_{lag}'] = df[col].shift(lag)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_39148\\1547269827.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_{lag}'] = df[col].shift(lag)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_39148\\1547269827.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_{lag}'] = df[col].shift(lag)\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„° ë¡œë“œ\n",
    "df = pd.read_csv('filled_merged_all_data.csv', encoding='utf-8')\n",
    "df['date'] = pd.to_datetime(df['ë‚ ì§œ'])\n",
    "\n",
    "# ê° ì¹¼ëŸ¼ë³„ lag ì¼ìˆ˜ ì •ì˜\n",
    "lag_days = {\n",
    "\n",
    "'ê´‘ì–´_KOSPI' : 136,\n",
    "'ê´‘ì–´_USD/KRW' : 1,\n",
    "'ê´‘ì–´_WTI' : 1,\n",
    "'ê´‘ì–´_VIX' : 399,\n",
    "'ê´‘ì–´_Gold' : 314,\n",
    "'ê´‘ì–´_Silver' : 238,\n",
    "'ê´‘ì–´_MOVE' : 18,\n",
    "\n",
    "'ë†ì–´_KOSPI' : 179,\n",
    "'ë†ì–´_USD/KRW' : 1,\n",
    "'ë†ì–´_WTI' : 100,\n",
    "'ë†ì–´_VIX' : 391,\n",
    "'ë†ì–´_Gold' : 361,\n",
    "'ë†ì–´_Silver' : 290,\n",
    "'ë†ì–´_MOVE' : 1,\n",
    "\n",
    "'ëŒ€ê²Œ_KOSPI' : 148,\n",
    "'ëŒ€ê²Œ_USD/KRW' : 90,\n",
    "'ëŒ€ê²Œ_WTI' : 91,\n",
    "# 'ëŒ€ê²Œ_VIX' : \n",
    "'ëŒ€ê²Œ_Gold' : 177,\n",
    "'ëŒ€ê²Œ_Silver' : 177,\n",
    "# 'ëŒ€ê²Œ_MOVE' : \n",
    "\n",
    "'ë°©ì–´_KOSPI' : 282,\n",
    "'ë°©ì–´_USD/KRW' : 387,\n",
    "'ë°©ì–´_WTI' : 399,\n",
    "'ë°©ì–´_VIX' : 133,\n",
    "'ë°©ì–´_Gold' : 1,\n",
    "'ë°©ì–´_Silver' : 1,\n",
    "'ë°©ì–´_MOVE' : 381,\n",
    "\n",
    "'ì—°ì–´_KOSPI' : 329,\n",
    "'ì—°ì–´_USD/KRW' : 1,\n",
    "'ì—°ì–´_WTI' : 199,\n",
    "'ì—°ì–´_VIX' : 399,\n",
    "'ì—°ì–´_Gold' : 1,\n",
    "'ì—°ì–´_Silver' : 399,\n",
    "'ì—°ì–´_MOVE' : 71,\n",
    "\n",
    "'ìš°ëŸ­_KOSPI' : 155,\n",
    "'ìš°ëŸ­_USD/KRW' : 121,\n",
    "'ìš°ëŸ­_WTI' : 14,\n",
    "'ìš°ëŸ­_VIX' : 38,\n",
    "'ìš°ëŸ­_Gold' : 146,\n",
    "'ìš°ëŸ­_Silver' : 125,\n",
    "# 'ìš°ëŸ­_MOVE' : \n",
    "\n",
    "'ì°¸ë”_KOSPI' : 399,\n",
    "'ì°¸ë”_USD/KRW' : 11,\n",
    "'ì°¸ë”_WTI' : 150,\n",
    "# 'ì°¸ë”_VIX' : \n",
    "# 'ì°¸ë”_Gold' : \n",
    "# 'ì°¸ë”_Silver' : \n",
    "'ì°¸ë”_MOVE' : 201,\n",
    " \n",
    "'ê´‘ì–´_20ëŒ€' : 250,\n",
    "'ê´‘ì–´_30ëŒ€' : 317,\n",
    "'ê´‘ì–´_40ëŒ€' : 330,\n",
    "'ê´‘ì–´_50ëŒ€' : 395,\n",
    "'ê´‘ì–´_60ëŒ€ì´ìƒ' : 339,\n",
    "\n",
    "'ë†ì–´_20ëŒ€' : 305,\n",
    "'ë†ì–´_30ëŒ€' : 307,\n",
    "'ë†ì–´_40ëŒ€' : 309,\n",
    "'ë†ì–´_50ëŒ€' : 309,\n",
    "'ë†ì–´_60ëŒ€ì´ìƒ' : 273,\n",
    "\n",
    "'ëŒ€ê²Œ_20ëŒ€' : 273,\n",
    "'ëŒ€ê²Œ_30ëŒ€' : 370,\n",
    "'ëŒ€ê²Œ_40ëŒ€' : 5,\n",
    "'ëŒ€ê²Œ_50ëŒ€' : 5,\n",
    "'ëŒ€ê²Œ_60ëŒ€ì´ìƒ' : 5,\n",
    "\n",
    "'ë°©ì–´_20ëŒ€' : 22,\n",
    "'ë°©ì–´_30ëŒ€' : 26,\n",
    "'ë°©ì–´_40ëŒ€' : 256,\n",
    "'ë°©ì–´_50ëŒ€' : 256,\n",
    "'ë°©ì–´_60ëŒ€ì´ìƒ' : 26,\n",
    "\n",
    "'ì—°ì–´_20ëŒ€' : 20,\n",
    "'ì—°ì–´_30ëŒ€' : 15,\n",
    "'ì—°ì–´_40ëŒ€' : 21,\n",
    "'ì—°ì–´_50ëŒ€' : 15,\n",
    "'ì—°ì–´_60ëŒ€ì´ìƒ' : 397,\n",
    "\n",
    "'ìš°ëŸ­_20ëŒ€' : 22,\n",
    "'ìš°ëŸ­_30ëŒ€' : 354,\n",
    "'ìš°ëŸ­_40ëŒ€' : 354,\n",
    "'ìš°ëŸ­_50ëŒ€' : 220,\n",
    "'ìš°ëŸ­_60ëŒ€ì´ìƒ' : 219,\n",
    "\n",
    "'ì°¸ë”_20ëŒ€' : 197,\n",
    "'ì°¸ë”_30ëŒ€' : 167,\n",
    "'ì°¸ë”_40ëŒ€' : 278,\n",
    "'ì°¸ë”_50ëŒ€' : 193,\n",
    "'ì°¸ë”_60ëŒ€ì´ìƒ' : 14,\n",
    "\n",
    "\n",
    "'ê´‘ì–´_ê¸°ì˜¨_22105' : 97,\n",
    "'ê´‘ì–´_ìˆ˜ì˜¨_22107' : 79,\n",
    "'ê´‘ì–´_ìŠµë„_22186' : 349,\n",
    "'ê´‘ì–´_íŒŒì£¼ê¸°_22190' : 103,\n",
    "\n",
    "'ë†ì–´_ê¸°ì˜¨_22105' : 103,\n",
    "'ë†ì–´_ìˆ˜ì˜¨_22107' : 81,\n",
    "'ë†ì–´_ìŠµë„_22186' : 333,\n",
    "'ë†ì–´_íŒŒì£¼ê¸°_22190' : 115,\n",
    "\n",
    "'ëŒ€ê²Œ_ê¸°ì˜¨_22105' : 150,\n",
    "'ëŒ€ê²Œ_ìˆ˜ì˜¨_22188' : 140,\n",
    "'ëŒ€ê²Œ_ìŠµë„_22188' : 355,\n",
    "'ëŒ€ê²Œ_íŒŒì£¼ê¸°_22105' : 174,\n",
    "\n",
    "'ë°©ì–´_ê¸°ì˜¨_22190' : 114,\n",
    "'ë°©ì–´_ìˆ˜ì˜¨_22107' : 118,\n",
    "'ë°©ì–´_ìŠµë„_22190' : 158,\n",
    "'ë°©ì–´_íŒŒì£¼ê¸°_22105' : 191,\n",
    "\n",
    "'ì—°ì–´_ê¸°ì˜¨_22105' : 394,\n",
    "'ì—°ì–´_ìˆ˜ì˜¨_22107' : 341,\n",
    "'ì—°ì–´_ìŠµë„_22105' : 8,\n",
    "'ì—°ì–´_íŒŒì£¼ê¸°_22190' : 9,\n",
    "\n",
    "'ìš°ëŸ­_ê¸°ì˜¨_22186' : 118,\n",
    "'ìš°ëŸ­_ìˆ˜ì˜¨_22186' : 113,\n",
    "'ìš°ëŸ­_ìŠµë„_22190' : 159,\n",
    "'ìš°ëŸ­_íŒŒì£¼ê¸°_22189' : 172,\n",
    "\n",
    "'ì°¸ë”_ê¸°ì˜¨_22190' : 91,\n",
    "'ì°¸ë”_ìˆ˜ì˜¨_22107' : 72,\n",
    "'ì°¸ë”_ìŠµë„_22190' : 115,\n",
    "'ì°¸ë”_íŒŒì£¼ê¸°_22190' : 107\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "# ê° ì¹¼ëŸ¼ì— ëŒ€í•´ ì§€ì •ëœ lag ì ìš©\n",
    "for col, lag in lag_days.items():\n",
    "   if col in df.columns:\n",
    "       df[f'{col}_{lag}'] = df[col].shift(lag)\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "df.to_csv('timelagged_features.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6-2. lag_1 íŒŒì¼ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¡œë“œ\n",
    "df = pd.read_csv('filled_merged_all_data.csv', encoding='utf-8')\n",
    "df['date'] = pd.to_datetime(df['ë‚ ì§œ'])\n",
    "\n",
    "# ê° ì¹¼ëŸ¼ë³„ lag ì¼ìˆ˜ ì •ì˜\n",
    "lag_days = {\n",
    "\n",
    " \n",
    "'ê´‘ì–´_20ëŒ€' : 1,\n",
    "'ê´‘ì–´_30ëŒ€' : 1,\n",
    "'ê´‘ì–´_40ëŒ€' : 1,\n",
    "'ê´‘ì–´_50ëŒ€' : 1,\n",
    "'ê´‘ì–´_60ëŒ€ì´ìƒ' : 1,\n",
    "\n",
    "'ë†ì–´_20ëŒ€' : 1,\n",
    "'ë†ì–´_30ëŒ€' : 1,\n",
    "'ë†ì–´_40ëŒ€' : 1,\n",
    "'ë†ì–´_50ëŒ€' : 1,\n",
    "'ë†ì–´_60ëŒ€ì´ìƒ' : 1,\n",
    "\n",
    "'ëŒ€ê²Œ_20ëŒ€' : 1,\n",
    "'ëŒ€ê²Œ_30ëŒ€' : 1,\n",
    "'ëŒ€ê²Œ_40ëŒ€' : 1,\n",
    "'ëŒ€ê²Œ_50ëŒ€' : 1,\n",
    "'ëŒ€ê²Œ_60ëŒ€ì´ìƒ' : 1,\n",
    "\n",
    "'ë°©ì–´_20ëŒ€' : 1,\n",
    "'ë°©ì–´_30ëŒ€' : 1,\n",
    "'ë°©ì–´_40ëŒ€' : 1,\n",
    "'ë°©ì–´_50ëŒ€' : 1,\n",
    "'ë°©ì–´_60ëŒ€ì´ìƒ' : 1,\n",
    "\n",
    "'ì—°ì–´_20ëŒ€' : 1,\n",
    "'ì—°ì–´_30ëŒ€' : 1,\n",
    "'ì—°ì–´_40ëŒ€' : 1,\n",
    "'ì—°ì–´_50ëŒ€' : 1,\n",
    "'ì—°ì–´_60ëŒ€ì´ìƒ' : 1,\n",
    "\n",
    "'ìš°ëŸ­_20ëŒ€' : 1,\n",
    "'ìš°ëŸ­_30ëŒ€' : 1,\n",
    "'ìš°ëŸ­_40ëŒ€' : 1,\n",
    "'ìš°ëŸ­_50ëŒ€' : 1,\n",
    "'ìš°ëŸ­_60ëŒ€ì´ìƒ' : 1,\n",
    "\n",
    "'ì°¸ë”_20ëŒ€' : 1,\n",
    "'ì°¸ë”_30ëŒ€' : 1,\n",
    "'ì°¸ë”_40ëŒ€' : 1,\n",
    "'ì°¸ë”_50ëŒ€' : 1,\n",
    "'ì°¸ë”_60ëŒ€ì´ìƒ' : 1\n",
    "}\n",
    "\n",
    "# ê° ì¹¼ëŸ¼ì— ëŒ€í•´ ì§€ì •ëœ lag ì ìš©\n",
    "for col, lag in lag_days.items():\n",
    "   if col in df.columns:\n",
    "       df[f'{col}_{lag}'] = df[col].shift(lag)\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "df.to_csv('timelagged_features1.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6-3. íŠ¸ë Œë“œ ë°ì´í„° í•©ì¹˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… ë³€ê²½ëœ ë°ì´í„° (ìƒìœ„ 5ê°œ í–‰):\n",
      "         date  ê´‘ì–´_20ëŒ€_1  ê´‘ì–´_20ëŒ€_250  ê´‘ì–´_30ëŒ€_1  ê´‘ì–´_30ëŒ€_317  ê´‘ì–´_40ëŒ€_1  \\\n",
      "0  2015-01-01       NaN         NaN       NaN         NaN       NaN   \n",
      "1  2015-01-02       NaN         NaN       NaN         NaN       NaN   \n",
      "2  2015-01-03       NaN         NaN       NaN         NaN       NaN   \n",
      "3  2015-01-04       NaN         NaN       NaN         NaN       NaN   \n",
      "4  2015-01-05       NaN         NaN       NaN         NaN       NaN   \n",
      "\n",
      "   ê´‘ì–´_40ëŒ€_330  ê´‘ì–´_50ëŒ€_1  ê´‘ì–´_50ëŒ€_395  ê´‘ì–´_60ëŒ€ì´ìƒ_1  ...  ì°¸ë”_60ëŒ€ì´ìƒ_1  ì°¸ë”_60ëŒ€ì´ìƒ_14  \\\n",
      "0         NaN       NaN         NaN         NaN  ...         NaN          NaN   \n",
      "1         NaN       NaN         NaN         NaN  ...         NaN          NaN   \n",
      "2         NaN       NaN         NaN         NaN  ...         NaN          NaN   \n",
      "3         NaN       NaN         NaN         NaN  ...         NaN          NaN   \n",
      "4         NaN       NaN         NaN         NaN  ...         NaN          NaN   \n",
      "\n",
      "   ì°¸ë”_KOSPI_399  ì°¸ë”_MOVE_201  ì°¸ë”_USD/KRW_11  ì°¸ë”_WTI_150  ì°¸ë”_ê¸°ì˜¨_22190_91  \\\n",
      "0           NaN          NaN            NaN         NaN             NaN   \n",
      "1           NaN          NaN            NaN         NaN             NaN   \n",
      "2           NaN          NaN            NaN         NaN             NaN   \n",
      "3           NaN          NaN            NaN         NaN             NaN   \n",
      "4           NaN          NaN            NaN         NaN             NaN   \n",
      "\n",
      "   ì°¸ë”_ìˆ˜ì˜¨_22107_72  ì°¸ë”_ìŠµë„_22190_115  ì°¸ë”_íŒŒì£¼ê¸°_22190_107  \n",
      "0             NaN              NaN               NaN  \n",
      "1             NaN              NaN               NaN  \n",
      "2             NaN              NaN               NaN  \n",
      "3             NaN              NaN               NaN  \n",
      "4             NaN              NaN               NaN  \n",
      "\n",
      "[5 rows x 142 columns]\n",
      "\n",
      "ğŸ“ ì²˜ë¦¬ëœ íŒŒì¼ì´ 'final_timelagged_features.csv'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "file_path1 = \"timelagged_features.csv\"\n",
    "file_path2 = \"timelagged_features1.csv\"\n",
    "output_file = \"final_timelagged_features.csv\"  # ì €ì¥ë  íŒŒì¼\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "df1 = pd.read_csv(file_path1)\n",
    "df2 = pd.read_csv(file_path2)\n",
    "\n",
    "# ê³µí†µ ì¹¼ëŸ¼ í™•ì¸\n",
    "common_columns = set(df1.columns) & set(df2.columns)\n",
    "\n",
    "# ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ì¹¼ëŸ¼ë§Œ ì„ íƒ\n",
    "unique_columns_df1 = set(df1.columns) - common_columns\n",
    "unique_columns_df2 = set(df2.columns) - common_columns\n",
    "\n",
    "# 'date' ì¹¼ëŸ¼ í¬í•¨í•˜ì—¬ ìƒˆë¡œìš´ ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "final_columns = ['date'] + list(unique_columns_df1) + list(unique_columns_df2)\n",
    "df_final = pd.concat([df1[['date']], df1[list(unique_columns_df1)], df2[list(unique_columns_df2)]], axis=1)\n",
    "\n",
    "# 'date' ì¹¼ëŸ¼ì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ì¹¼ëŸ¼ì„ ê°€ë‚˜ë‹¤ ìˆœìœ¼ë¡œ ì •ë ¬\n",
    "sorted_columns = sorted([col for col in df_final.columns if col != 'date'])\n",
    "\n",
    "# 'date' ì¹¼ëŸ¼ì„ ì•ì— ë‘ê³  ì •ë ¬ëœ ì¹¼ëŸ¼ ë°°ì¹˜\n",
    "df_final = df_final[['date'] + sorted_columns]\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "df_final.to_csv(output_file, index=False)\n",
    "\n",
    "# ë³€ê²½ëœ ë°ì´í„° í™•ì¸\n",
    "print(\"\\nâœ… ë³€ê²½ëœ ë°ì´í„° (ìƒìœ„ 5ê°œ í–‰):\")\n",
    "print(df_final.head())\n",
    "\n",
    "print(f\"\\nğŸ“ ì²˜ë¦¬ëœ íŒŒì¼ì´ '{output_file}'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6-4. ì–´ì¢…ë³„ íƒ€ì„ë˜ê·¸ ë°ì´í„° ë¶„ë¥˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created ê´‘ì–´_timelagged_features.csv with 22 columns\n",
      "Created ë†ì–´_timelagged_features.csv with 22 columns\n",
      "Created ëŒ€ê²Œ_timelagged_features.csv with 20 columns\n",
      "Created ë°©ì–´_timelagged_features.csv with 22 columns\n",
      "Created ì—°ì–´_timelagged_features.csv with 22 columns\n",
      "Created ìš°ëŸ­_timelagged_features.csv with 21 columns\n",
      "Created ì°¸ë”_timelagged_features.csv with 19 columns\n"
     ]
    }
   ],
   "source": [
    "# ì–´ì¢…ë³„ ë¶„ë¥˜ \n",
    "\n",
    "def split_by_fish():\n",
    "    df = pd.read_csv('final_timelagged_features.csv', encoding='utf-8')\n",
    "    fish_types = ['ê´‘ì–´', 'ë†ì–´', 'ëŒ€ê²Œ', 'ë°©ì–´', 'ì—°ì–´', 'ìš°ëŸ­', 'ì°¸ë”']\n",
    "    \n",
    "    for fish in fish_types:\n",
    "        # Get date and fish-specific columns\n",
    "        fish_cols = ['date'] + [col for col in df.columns if fish in col]\n",
    "        fish_df = df[fish_cols]\n",
    "        \n",
    "        output_file = f'{fish}_timelagged_features.csv'\n",
    "        fish_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "        print(f'Created {output_file} with {len(fish_cols)} columns')\n",
    "\n",
    "split_by_fish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. ê°€ê²© ë°ì´í„° \n",
    "\n",
    "* ì…ë ¥ íŒŒì¼ : 'ikh_item_price_2025-01-30.csv\n",
    "* ì¶œë ¥ íŒŒì¼ : 'item_price_lag.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7-1. ë…¸ëŸ‰ì§„ 2ì¸µ ë²„ë¦¬ê¸°\n",
    "\n",
    "* ì…ë ¥ íŒŒì¼ : 'ikh_item_price_2025-01-30.csv\n",
    "* ì¶œë ¥ íŒŒì¼ : 'item_price_deleted.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… ë³€ê²½ëœ ë°ì´í„° (ìƒìœ„ 5ê°œ í–‰):\n",
      "    priceDate  minPrice  avgPrice  maxPrice item market\n",
      "0  2015-03-06     20000     20000     20000   ë†ì–´  ë…¸ëŸ‰ì§„ì‹œì¥\n",
      "1  2015-03-14     25000     25000     25000   ë†ì–´  ë…¸ëŸ‰ì§„ì‹œì¥\n",
      "2  2015-04-18     25000     27500     30000   ë†ì–´  ë…¸ëŸ‰ì§„ì‹œì¥\n",
      "3  2015-04-28     25000     25000     25000   ë†ì–´  ë…¸ëŸ‰ì§„ì‹œì¥\n",
      "4  2015-05-18     25000     25000     25000   ë†ì–´  ë…¸ëŸ‰ì§„ì‹œì¥\n",
      "\n",
      "ğŸ“ ì²˜ë¦¬ëœ íŒŒì¼ì´ 'item_price_deleted.csv'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "file_path = \"ikh_item_price_2025-01-30.csv\"  # ì›ë³¸ ë°ì´í„° íŒŒì¼\n",
    "output_file = \"item_price_deleted.csv\"  # ì €ì¥ë  íŒŒì¼\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 'market' ì¹¼ëŸ¼ì—ì„œ 'ë…¸ëŸ‰ì§„ 2ì¸µ'ì¸ í–‰ ì œê±°\n",
    "df = df[df['market'] != 'ë…¸ëŸ‰ì§„ 2ì¸µ']\n",
    "\n",
    "# 'ë…¸ëŸ‰ì§„ 1ì¸µ'ì„ 'ë…¸ëŸ‰ì§„ì‹œì¥'ìœ¼ë¡œ ë³€ê²½\n",
    "df['market'] = df['market'].replace('ë…¸ëŸ‰ì§„ 1ì¸µ', 'ë…¸ëŸ‰ì§„ì‹œì¥')\n",
    "\n",
    "# ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "# ë³€ê²½ëœ ë°ì´í„° í™•ì¸\n",
    "print(\"\\nâœ… ë³€ê²½ëœ ë°ì´í„° (ìƒìœ„ 5ê°œ í–‰):\")\n",
    "print(df.head())\n",
    "\n",
    "print(f\"\\nğŸ“ ì²˜ë¦¬ëœ íŒŒì¼ì´ '{output_file}'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7-2. ëª¨ë“  ë‚ ì§œ ìƒì„± ë° ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°\n",
    "\n",
    "* ì…ë ¥ íŒŒì¼ : 'item_price_deleted.csv\n",
    "* ì¶œë ¥ íŒŒì¼ : 'item_price_filled_deleted.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„°ê°€ item_price_filled_deleted.csv íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ë°ì´í„° íŒŒì¼ ë¡œë“œ\n",
    "file_path = \"item_price_deleted.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# ë‚ ì§œ ë°ì´í„° ë³€í™˜\n",
    "df[\"priceDate\"] = pd.to_datetime(df[\"priceDate\"])\n",
    "\n",
    "# itemê³¼ marketì˜ ì¡°í•©ë³„ë¡œ ì²˜ë¦¬\n",
    "filled_dfs = []\n",
    "\n",
    "for (item, market), group in df.groupby([\"item\", \"market\"]):\n",
    "    # ì „ì²´ ë‚ ì§œ ë²”ìœ„ ìƒì„±\n",
    "    full_date_range = pd.date_range(start=group[\"priceDate\"].min(), end=group[\"priceDate\"].max())\n",
    "\n",
    "    # ê¸°ì¡´ ë°ì´í„°í”„ë ˆì„ì„ ë‚ ì§œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¦¬ì¸ë±ì‹±\n",
    "    group = group.set_index(\"priceDate\").reindex(full_date_range).reset_index()\n",
    "    group.rename(columns={\"index\": \"priceDate\"}, inplace=True)\n",
    "\n",
    "    # itemê³¼ market ì •ë³´ ì±„ìš°ê¸°\n",
    "    group[\"item\"] = item\n",
    "    group[\"market\"] = market\n",
    "\n",
    "    # ê²°ì¸¡ê°’ì„ ì•ì˜ ê°’ìœ¼ë¡œ ì±„ìš°ê¸° (ffill)\n",
    "    group.ffill(inplace=True)\n",
    "\n",
    "    # ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "    filled_dfs.append(group)\n",
    "\n",
    "# ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì¹¨\n",
    "filled_df = pd.concat(filled_dfs, ignore_index=True)\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥ (ì›í•˜ëŠ” ê²½ë¡œë¡œ ë³€ê²½ ê°€ëŠ¥)\n",
    "output_path = \"item_price_filled_deleted.csv\"\n",
    "filled_df.to_csv(output_path, index=False)\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸\n",
    "print(f\"ë°ì´í„°ê°€ {output_path} íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7-3. 1ì¼ íƒ€ì„ë˜ê·¸ ìƒì„±\n",
    "\n",
    "* ì…ë ¥ íŒŒì¼ : 'item_price_filled_deleted.csv\n",
    "* ì¶œë ¥ íŒŒì¼ : 'item_price_lag_filled_deleted.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ (íŒŒì¼ ê²½ë¡œëŠ” ì ì ˆí•˜ê²Œ ìˆ˜ì •)\n",
    "file_path = \"item_price_filled_deleted.csv\"\n",
    "output_file = \"item_price_lag_filled_deleted.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# ë‚ ì§œ ì¹¼ëŸ¼ ì´ë¦„ì„ 'priceDate'ì—ì„œ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "df['priceDate'] = pd.to_datetime(df['priceDate'])\n",
    "\n",
    "# avgPriceì— 1ì¼ íƒ€ì„ë˜ê·¸ ì ìš©\n",
    "df['avgPrice_lag_1'] = df['avgPrice'].shift(1)\n",
    "\n",
    "# ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥\n",
    "df.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. ì‹œì¥ ë°ì´í„° ì›í•« ì¸ì½”ë”©\n",
    "\n",
    "* ì…ë ¥ íŒŒì¼ : 'item_price_lag_filled_deleted.csv'\n",
    "* ì¶œë ¥ íŒŒì¼ : 'item_price_oneHot.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import DateTime\n",
    "\n",
    "def transform_market_data(file_path):\n",
    "    # ë°ì´í„° ì½ê¸°\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # ì‹œì¥ë³„ ë”ë¯¸ë³€ìˆ˜ ìƒì„± (0, 1ë¡œ ì¸ì½”ë”©)\n",
    "    market_dummies = pd.get_dummies(df['market'], prefix='m').astype(int)\n",
    "    \n",
    "     # priceDate ì¹¼ëŸ¼ëª…ì„ dateë¡œ ë³€ê²½\n",
    "    df = df.rename(columns={'priceDate': 'date'})\n",
    "    \n",
    "    # ì›ë³¸ ë°ì´í„°ì™€ ë”ë¯¸ë³€ìˆ˜ ê²°í•©\n",
    "    result = pd.concat([\n",
    "        df[['date', 'item']],  # 'priceDate' â†’ 'date'ë¡œ ë³€ê²½ë¨\n",
    "        market_dummies,\n",
    "        df[['avgPrice', 'avgPrice_lag_1']]\n",
    "    ], axis=1)\n",
    "    \n",
    "    # ë‚ ì§œì™€ ì–´ì¢…ìœ¼ë¡œ ì •ë ¬\n",
    "    result = result.sort_values(['date', 'item'])\n",
    "    \n",
    "    # ë³€í™˜ëœ ë°ì´í„° ì €ì¥\n",
    "    output_file = 'item_price_oneHot.csv'\n",
    "    result.to_csv(output_file, index=False)\n",
    "    print(f\"âœ… ìƒì„±ëœ íŒŒì¼: {output_file}\")\n",
    "    print(\"\\nğŸ” ì²˜ìŒ 10ê°œ ì»¬ëŸ¼:\", result.columns[:10].tolist())\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ìƒì„±ëœ íŒŒì¼: item_price_oneHot.csv\n",
      "\n",
      "ğŸ” ì²˜ìŒ 10ê°œ ì»¬ëŸ¼: ['date', 'item', 'm_ê°€ë½ì‹œì¥', 'm_ê°•ì„œë†ìˆ˜ì‚°ë¬¼ì‹œì¥', 'm_êµ¬ë¦¬ë†ìˆ˜ì‚°ë¬¼ì‹œì¥', 'm_ë…¸ëŸ‰ì§„ì‹œì¥', 'm_ë§ˆí¬ë†ìˆ˜ì‚°ë¬¼ì‹œì¥', 'm_ë¶€ì‚°ë¯¼ë½ì–´ë¯¼í™œì–´ì§íŒì¥', 'm_ì†Œë˜í¬êµ¬ì¢…í•©ì–´ì‹œì¥', 'm_ìˆ˜ì›ë†ìˆ˜ì‚°ë¬¼ì‹œì¥']\n"
     ]
    }
   ],
   "source": [
    "# Execute\n",
    "df = transform_market_data('item_price_lag_filled_deleted.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. ê°€ê²©ë°ì´í„° ì–´ì¢…ë³„ë¡œ ë‚˜ëˆ„ê¸°\n",
    "\n",
    "* ì…ë ¥ íŒŒì¼ : 'item_price_oneHot.csv'\n",
    "* ì¶œë ¥ íŒŒì¼ : '{fish}_price_oneHot.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def split_market_data():\n",
    "    df = pd.read_csv('item_price_oneHot.csv')\n",
    "    \n",
    "    for fish in df['item'].unique():\n",
    "        fish_df = df[df['item'] == fish]\n",
    "        output_file = f'{fish}_price_oneHot.csv'\n",
    "      # fish_df = fish_df.drop('item', axis=1)\n",
    "        fish_df.to_csv(output_file, index=False)\n",
    "        print(f'{fish} ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(fish_df)}ê°œ í–‰')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëŒ€ê²Œ ë°ì´í„° ìƒì„± ì™„ë£Œ: 24131ê°œ í–‰\n",
      "ê´‘ì–´ ë°ì´í„° ìƒì„± ì™„ë£Œ: 29489ê°œ í–‰\n",
      "ë†ì–´ ë°ì´í„° ìƒì„± ì™„ë£Œ: 28543ê°œ í–‰\n",
      "ì—°ì–´ ë°ì´í„° ìƒì„± ì™„ë£Œ: 27468ê°œ í–‰\n",
      "ì°¸ë” ë°ì´í„° ìƒì„± ì™„ë£Œ: 25041ê°œ í–‰\n",
      "ë°©ì–´ ë°ì´í„° ìƒì„± ì™„ë£Œ: 9715ê°œ í–‰\n",
      "ìš°ëŸ­ ë°ì´í„° ìƒì„± ì™„ë£Œ: 5641ê°œ í–‰\n"
     ]
    }
   ],
   "source": [
    "split_market_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. ìµœì¢… ë°ì´í„° í•©ì¹˜ê¸°\n",
    "\n",
    "* ì…ë ¥ íŒŒì¼ : '{fish}_price_oneHot.csv', '{fish}_timelagged_features.csv'\n",
    "* ì¶œë ¥ íŒŒì¼ : '{fish}_price_features.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def merge_features_price(fish):\n",
    "    # ë‘ ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "    features_df = pd.read_csv(f'{fish}_timelagged_features.csv')\n",
    "    price_df = pd.read_csv(f'{fish}_price_oneHot.csv')\n",
    "    \n",
    "    # ë‚ ì§œ ì»¬ëŸ¼ëª… í†µì¼\n",
    "    price_df = price_df.rename(columns={'dmdpriceDate': 'date'})\n",
    "    \n",
    "    # ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„° ë³‘í•©\n",
    "    merged_df = pd.merge(price_df, features_df, on='date', how='left')\n",
    "    \n",
    "    # CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "    output_file = f'{fish}_price_features.csv'\n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f'ìƒì„±ëœ íŒŒì¼: {output_file}')\n",
    "    print(f'ì „ì²´ ì»¬ëŸ¼ ìˆ˜: {len(merged_df.columns)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìƒì„±ëœ íŒŒì¼: ê´‘ì–´_price_features.csv\n",
      "ì „ì²´ ì»¬ëŸ¼ ìˆ˜: 35\n",
      "ìƒì„±ëœ íŒŒì¼: ë†ì–´_price_features.csv\n",
      "ì „ì²´ ì»¬ëŸ¼ ìˆ˜: 35\n",
      "ìƒì„±ëœ íŒŒì¼: ëŒ€ê²Œ_price_features.csv\n",
      "ì „ì²´ ì»¬ëŸ¼ ìˆ˜: 33\n",
      "ìƒì„±ëœ íŒŒì¼: ë°©ì–´_price_features.csv\n",
      "ì „ì²´ ì»¬ëŸ¼ ìˆ˜: 35\n",
      "ìƒì„±ëœ íŒŒì¼: ì—°ì–´_price_features.csv\n",
      "ì „ì²´ ì»¬ëŸ¼ ìˆ˜: 35\n",
      "ìƒì„±ëœ íŒŒì¼: ìš°ëŸ­_price_features.csv\n",
      "ì „ì²´ ì»¬ëŸ¼ ìˆ˜: 34\n",
      "ìƒì„±ëœ íŒŒì¼: ì°¸ë”_price_features.csv\n",
      "ì „ì²´ ì»¬ëŸ¼ ìˆ˜: 32\n"
     ]
    }
   ],
   "source": [
    "fish_list = ['ê´‘ì–´', 'ë†ì–´', 'ëŒ€ê²Œ', 'ë°©ì–´', 'ì—°ì–´', 'ìš°ëŸ­', 'ì°¸ë”']\n",
    "\n",
    "for fish in fish_list:\n",
    "    merge_features_price(fish)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. ìµœì¢… ê²°ì¸¡ì¹˜ ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11-1. ê²°ì¸¡ì¹˜ ì—†ëŠ” í–‰ë§Œ ë‚¨ê¸°ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def clean_fish_data(fish_list, input_path, output_path):\n",
    "    \"\"\"\n",
    "    ì—¬ëŸ¬ ìˆ˜ì‚°ë¬¼ ë°ì´í„°ì—ì„œ NULL ê°’ì´ ì—†ëŠ” ì²« ë²ˆì§¸ í–‰ë¶€í„° ë§ˆì§€ë§‰ í–‰ê¹Œì§€ í•„í„°ë§í•˜ì—¬ ì €ì¥í•˜ëŠ” í•¨ìˆ˜.\n",
    "\n",
    "    Args:\n",
    "        fish_list (list): ìˆ˜ì‚°ë¬¼ ëª©ë¡ (ì˜ˆ: ['ê´‘ì–´', 'ë†ì–´', ...])\n",
    "        input_path (str): ì›ë³¸ ë°ì´í„° ê²½ë¡œ íŒ¨í„´ (ì˜ˆ: '{fish}_price_features.csv')\n",
    "        output_path (str): ì €ì¥ë  íŒŒì¼ ê²½ë¡œ íŒ¨í„´ (ì˜ˆ: 'cleaned_{fish}_price_features.csv')\n",
    "    \"\"\"\n",
    "    for fish in fish_list:\n",
    "        try:\n",
    "            # íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "            input_file = input_path.format(fish=fish)\n",
    "            output_file = output_path.format(fish=fish)\n",
    "\n",
    "            # ë°ì´í„° ë¡œë“œ\n",
    "            df = pd.read_csv(input_file)\n",
    "\n",
    "\n",
    "            # ì²« ë²ˆì§¸ë¡œ NULL ê°’ì´ ì—†ëŠ” í–‰ ì°¾ê¸°\n",
    "            first_valid_index = df.dropna().index[1]\n",
    "\n",
    "            # ë§ˆì§€ë§‰ìœ¼ë¡œ NULL ê°’ì´ ì—†ëŠ” í–‰ ì°¾ê¸°\n",
    "            last_valid_index = df.dropna().index[-1]\n",
    "\n",
    "            # NULL ê°’ì´ ì—†ëŠ” ì²« ë²ˆì§¸ í–‰ë¶€í„° ë§ˆì§€ë§‰ í–‰ê¹Œì§€ ë°ì´í„° ì„ íƒ\n",
    "            df_cleaned = df.loc[first_valid_index:last_valid_index]\n",
    "\n",
    "            # ê²°ê³¼ ì €ì¥\n",
    "            df_cleaned.to_csv(output_file, index=False)\n",
    "\n",
    "            print(f\"âœ… ìƒì„±ëœ íŒŒì¼: {output_file} (ì „ì²´ í–‰ ìˆ˜: {len(df_cleaned)})\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ {fish} ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ìƒì„±ëœ íŒŒì¼: notnull_ê´‘ì–´_price_features.csv (ì „ì²´ í–‰ ìˆ˜: 26555)\n",
      "âœ… ìƒì„±ëœ íŒŒì¼: notnull_ë†ì–´_price_features.csv (ì „ì²´ í–‰ ìˆ˜: 25286)\n",
      "âœ… ìƒì„±ëœ íŒŒì¼: notnull_ëŒ€ê²Œ_price_features.csv (ì „ì²´ í–‰ ìˆ˜: 21516)\n",
      "âœ… ìƒì„±ëœ íŒŒì¼: notnull_ë°©ì–´_price_features.csv (ì „ì²´ í–‰ ìˆ˜: 9708)\n",
      "âœ… ìƒì„±ëœ íŒŒì¼: notnull_ì—°ì–´_price_features.csv (ì „ì²´ í–‰ ìˆ˜: 23975)\n",
      "âœ… ìƒì„±ëœ íŒŒì¼: notnull_ìš°ëŸ­_price_features.csv (ì „ì²´ í–‰ ìˆ˜: 5630)\n",
      "âœ… ìƒì„±ëœ íŒŒì¼: notnull_ì°¸ë”_price_features.csv (ì „ì²´ í–‰ ìˆ˜: 23354)\n"
     ]
    }
   ],
   "source": [
    "#  ëª¨ë“  ìˆ˜ì‚°ë¬¼ì— ëŒ€í•´ ë°˜ë³µ ì‹¤í–‰\n",
    "fish_list = ['ê´‘ì–´', 'ë†ì–´', 'ëŒ€ê²Œ', 'ë°©ì–´', 'ì—°ì–´', 'ìš°ëŸ­', 'ì°¸ë”']\n",
    "input_path = \"{fish}_price_features.csv\"\n",
    "output_path = \"notnull_{fish}_price_features.csv\"  # {fish}_price_features_notnull.csv\n",
    "\n",
    "clean_fish_data(fish_list, input_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11-2. ì‹œì¥ë³„ ì²˜ìŒ íŒë§¤ì¼ ì‚­ì œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ê´‘ì–´: ê²°ì¸¡ì¹˜ ìˆëŠ” í–‰ ì œê±° ì™„ë£Œ (ë‚¨ì€ í–‰ ìˆ˜: 26555)\n",
      "âœ… ë†ì–´: ê²°ì¸¡ì¹˜ ìˆëŠ” í–‰ ì œê±° ì™„ë£Œ (ë‚¨ì€ í–‰ ìˆ˜: 25286)\n",
      "âœ… ëŒ€ê²Œ: ê²°ì¸¡ì¹˜ ìˆëŠ” í–‰ ì œê±° ì™„ë£Œ (ë‚¨ì€ í–‰ ìˆ˜: 21516)\n",
      "âœ… ë°©ì–´: ê²°ì¸¡ì¹˜ ìˆëŠ” í–‰ ì œê±° ì™„ë£Œ (ë‚¨ì€ í–‰ ìˆ˜: 9708)\n",
      "âœ… ì—°ì–´: ê²°ì¸¡ì¹˜ ìˆëŠ” í–‰ ì œê±° ì™„ë£Œ (ë‚¨ì€ í–‰ ìˆ˜: 23975)\n",
      "âœ… ìš°ëŸ­: ê²°ì¸¡ì¹˜ ìˆëŠ” í–‰ ì œê±° ì™„ë£Œ (ë‚¨ì€ í–‰ ìˆ˜: 5630)\n",
      "âœ… ì°¸ë”: ê²°ì¸¡ì¹˜ ìˆëŠ” í–‰ ì œê±° ì™„ë£Œ (ë‚¨ì€ í–‰ ìˆ˜: 23354)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def remove_missing_values(fish_list, file_path_pattern):\n",
    "    \"\"\"\n",
    "    ì—¬ëŸ¬ ìˆ˜ì‚°ë¬¼ ë°ì´í„°ì—ì„œ ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” í–‰ì„ ì‚­ì œí•˜ê³  ê¸°ì¡´ íŒŒì¼ì— ë®ì–´ì“°ëŠ” í•¨ìˆ˜.\n",
    "\n",
    "    Args:\n",
    "        fish_list (list): ìˆ˜ì‚°ë¬¼ ëª©ë¡ (ì˜ˆ: ['ê´‘ì–´', 'ë†ì–´', ...])\n",
    "        file_path_pattern (str): ì²˜ë¦¬í•  íŒŒì¼ ê²½ë¡œ íŒ¨í„´ (ì˜ˆ: 'notnull_{fish}_price_features.csv')\n",
    "    \"\"\"\n",
    "    for fish in fish_list:\n",
    "        try:\n",
    "            # íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "            file_path = file_path_pattern.format(fish=fish)\n",
    "\n",
    "            # ë°ì´í„° ë¡œë“œ\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” í–‰ ì œê±°\n",
    "            df_cleaned = df.dropna()\n",
    "\n",
    "            # ê¸°ì¡´ íŒŒì¼ì— ë®ì–´ì“°ê¸° ì €ì¥\n",
    "            df_cleaned.to_csv(file_path, index=False)\n",
    "\n",
    "            print(f\"âœ… {fish}: ê²°ì¸¡ì¹˜ ìˆëŠ” í–‰ ì œê±° ì™„ë£Œ (ë‚¨ì€ í–‰ ìˆ˜: {len(df_cleaned)})\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ {fish} ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "# âœ… ëª¨ë“  ìˆ˜ì‚°ë¬¼ì— ëŒ€í•´ ì‹¤í–‰\n",
    "fish_list = ['ê´‘ì–´', 'ë†ì–´', 'ëŒ€ê²Œ', 'ë°©ì–´', 'ì—°ì–´', 'ìš°ëŸ­', 'ì°¸ë”']\n",
    "file_path_pattern = \"notnull_{fish}_price_features.csv\"\n",
    "\n",
    "remove_missing_values(fish_list, file_path_pattern)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11-1. ê²°ì¸¡ì¹˜ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ê´‘ì–´: ê²°ì¸¡ì¹˜ ì—†ìŒ\n",
      "âœ… ë†ì–´: ê²°ì¸¡ì¹˜ ì—†ìŒ\n",
      "âœ… ëŒ€ê²Œ: ê²°ì¸¡ì¹˜ ì—†ìŒ\n",
      "âœ… ë°©ì–´: ê²°ì¸¡ì¹˜ ì—†ìŒ\n",
      "âœ… ì—°ì–´: ê²°ì¸¡ì¹˜ ì—†ìŒ\n",
      "âœ… ìš°ëŸ­: ê²°ì¸¡ì¹˜ ì—†ìŒ\n",
      "âœ… ì°¸ë”: ê²°ì¸¡ì¹˜ ì—†ìŒ\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def check_missing_values(fish_list, file_path_pattern):\n",
    "    \"\"\"\n",
    "    ì—¬ëŸ¬ ìˆ˜ì‚°ë¬¼ ë°ì´í„°ì—ì„œ ê²°ì¸¡ì¹˜ ì—¬ë¶€ë¥¼ ê²€ì‚¬í•˜ëŠ” í•¨ìˆ˜.\n",
    "\n",
    "    Args:\n",
    "        fish_list (list): ìˆ˜ì‚°ë¬¼ ëª©ë¡ (ì˜ˆ: ['ê´‘ì–´', 'ë†ì–´', ...])\n",
    "        file_path_pattern (str): ê²€ì‚¬í•  íŒŒì¼ ê²½ë¡œ íŒ¨í„´ (ì˜ˆ: 'notnull_{fish}_price_features.csv')\n",
    "    \"\"\"\n",
    "    for fish in fish_list:\n",
    "        try:\n",
    "            # íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "            file_path = file_path_pattern.format(fish=fish)\n",
    "\n",
    "            # ë°ì´í„° ë¡œë“œ\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # ê²°ì¸¡ì¹˜ í™•ì¸\n",
    "            missing_values = df.isnull().sum()\n",
    "            total_missing = missing_values.sum()\n",
    "\n",
    "            # ê²°ê³¼ ì¶œë ¥\n",
    "            if total_missing > 0:\n",
    "                print(f\"âš ï¸ {fish}: ì´ {total_missing}ê°œì˜ ê²°ì¸¡ì¹˜ ë°œê²¬!\")\n",
    "                print(missing_values[missing_values > 0])\n",
    "            else:\n",
    "                print(f\"âœ… {fish}: ê²°ì¸¡ì¹˜ ì—†ìŒ\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ {fish} ë°ì´í„° ê²€ì‚¬ ì‹¤íŒ¨: {e}\")\n",
    "            \n",
    "# âœ… ëª¨ë“  ìˆ˜ì‚°ë¬¼ì— ëŒ€í•´ ê²°ì¸¡ì¹˜ ê²€ì‚¬ ì‹¤í–‰\n",
    "fish_list = ['ê´‘ì–´', 'ë†ì–´', 'ëŒ€ê²Œ', 'ë°©ì–´', 'ì—°ì–´', 'ìš°ëŸ­', 'ì°¸ë”']\n",
    "file_path_pattern = \"notnull_{fish}_price_features.csv\"\n",
    "\n",
    "check_missing_values(fish_list, file_path_pattern)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. ì¤‘ê°„ ê³¼ì • íŒŒì¼ ì‚­ì œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted: _economic_indicators_.csv\n",
      "File not found: filled_economic_indicators_.csv\n",
      "File not found: expanded_economic_indicators_.csv\n",
      "Deleted: ê·¸ë£¹í™”_nst_ì°¸ë”_trend.csv\n",
      "Deleted: merged_trends.csv\n",
      "Deleted: weatherdata_processed.csv\n",
      "Deleted: merged_all_data.csv\n",
      "Deleted: filled_merged_all_data.csv\n",
      "Deleted: timelagged_features.csv\n",
      "Deleted: timelagged_features1.csv\n",
      "Deleted: final_timelagged_features.csv\n",
      "Deleted: ì°¸ë”_timelagged_features.csv\n",
      "File not found: item_price_lag.csv\n",
      "File not found: item_price_lag_deleted.csv\n",
      "Deleted: item_price_lag_filled_deleted.csv\n",
      "Deleted: item_price_oneHot.csv\n",
      "Deleted: ì°¸ë”_price_oneHot.csv\n",
      "Deleted: ì°¸ë”_price_features.csv\n",
      "Deleted: ê·¸ë£¹í™”_nst_ê´‘ì–´_trend.csv\n",
      "Deleted: ê´‘ì–´_timelagged_features.csv\n",
      "Deleted: ê´‘ì–´_price_oneHot.csv\n",
      "Deleted: ê´‘ì–´_price_features.csv\n",
      "Deleted: ê·¸ë£¹í™”_nst_ë†ì–´_trend.csv\n",
      "Deleted: ë†ì–´_timelagged_features.csv\n",
      "Deleted: ë†ì–´_price_oneHot.csv\n",
      "Deleted: ë†ì–´_price_features.csv\n",
      "Deleted: ê·¸ë£¹í™”_nst_ëŒ€ê²Œ_trend.csv\n",
      "Deleted: ëŒ€ê²Œ_timelagged_features.csv\n",
      "Deleted: ëŒ€ê²Œ_price_oneHot.csv\n",
      "Deleted: ëŒ€ê²Œ_price_features.csv\n",
      "Deleted: ê·¸ë£¹í™”_nst_ë°©ì–´_trend.csv\n",
      "Deleted: ë°©ì–´_timelagged_features.csv\n",
      "Deleted: ë°©ì–´_price_oneHot.csv\n",
      "Deleted: ë°©ì–´_price_features.csv\n",
      "Deleted: ê·¸ë£¹í™”_nst_ì—°ì–´_trend.csv\n",
      "Deleted: ì—°ì–´_timelagged_features.csv\n",
      "Deleted: ì—°ì–´_price_oneHot.csv\n",
      "Deleted: ì—°ì–´_price_features.csv\n",
      "Deleted: ê·¸ë£¹í™”_nst_ìš°ëŸ­_trend.csv\n",
      "Deleted: ìš°ëŸ­_timelagged_features.csv\n",
      "Deleted: ìš°ëŸ­_price_oneHot.csv\n",
      "Deleted: ìš°ëŸ­_price_features.csv\n",
      "File not found: ê·¸ë£¹í™”_nst_ì°¸ë”_trend.csv\n",
      "File not found: ì°¸ë”_timelagged_features.csv\n",
      "File not found: ì°¸ë”_price_oneHot.csv\n",
      "File not found: ì°¸ë”_price_features.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# ì‚­ì œí•  ì–´ì¢… ë¦¬ìŠ¤íŠ¸\n",
    "fish_list = ['ê´‘ì–´', 'ë†ì–´', 'ëŒ€ê²Œ', 'ë°©ì–´', 'ì—°ì–´', 'ìš°ëŸ­', 'ì°¸ë”']\n",
    "\n",
    "remove_list = [\n",
    "    '_economic_indicators_.csv',\n",
    "\t'filled_economic_indicators_.csv',\n",
    "\t'expanded_economic_indicators_.csv',\n",
    "\tf'ê·¸ë£¹í™”_nst_{fish}_trend.csv',\n",
    "\t'merged_trends.csv',\n",
    "\t'weatherdata_processed.csv',\n",
    "\t'merged_all_data.csv',\n",
    "\t'filled_merged_all_data.csv',\n",
    "\t'timelagged_features.csv',\n",
    "\t'timelagged_features1.csv',\n",
    "\t'final_timelagged_features.csv',\n",
    "\tf'{fish}_timelagged_features.csv',\n",
    "\t'item_price_lag.csv',\n",
    "\t'item_price_lag_deleted.csv',\n",
    "\t'item_price_lag_filled_deleted.csv',\n",
    "\t'item_price_oneHot.csv',\n",
    "\tf'{fish}_price_oneHot.csv',\n",
    "\tf'{fish}_price_features.csv'\n",
    "]\n",
    "\n",
    "# ì–´ì¢…(fish)ë³„ íŒŒì¼ ì¶”ê°€\n",
    "for fish in fish_list:\n",
    "    remove_list.extend([\n",
    "        f'ê·¸ë£¹í™”_nst_{fish}_trend.csv',\n",
    "        f'{fish}_timelagged_features.csv',\n",
    "        f'{fish}_price_oneHot.csv',\n",
    "        f'{fish}_price_features.csv',\n",
    "    ])\n",
    "\n",
    "# ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” íŒŒì¼ë“¤ì„ í•˜ë‚˜ì”© ì‚­ì œ\n",
    "for file in remove_list:\n",
    "    if os.path.exists(file):  # íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸\n",
    "        os.remove(file)\n",
    "        print(f\"Deleted: {file}\")\n",
    "    else:\n",
    "        print(f\"File not found: {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env311_cu121",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
