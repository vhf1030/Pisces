{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê²½ì œì§€í‘œ ë°ì´í„° ì „ì²˜ë¦¬\n",
    "\n",
    "filled_economic_indicators.csv ìƒì„±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### yfinanceì—ì„œ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ ì„¤ì •\n",
    "start_date = '2015-01-01'\n",
    "end_date = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# ë°ì´í„°ë¥¼ ì €ì¥í•  ë¹ˆ ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "df_final = pd.DataFrame()\n",
    "\n",
    "# ê° ì§€í‘œì˜ í‹°ì»¤ ì‹¬ë³¼ ì •ì˜\n",
    "tickers = {\n",
    "    'KOSPI': '^KS11',  # KOSPI\n",
    "    'USD/KRW': 'KRW=X',  # ì›ë‹¬ëŸ¬ í™˜ìœ¨\n",
    "    'WTI': 'CL=F',  # WTI ì›ìœ  ì„ ë¬¼\n",
    "    'VIX': '^VIX',  # VIX ì§€ìˆ˜\n",
    "    'Gold': 'GC=F',  # ê¸ˆ ì„ ë¬¼\n",
    "    'Silver': 'SI=F',  # ì€ ì„ ë¬¼\n",
    "    'MOVE' : '^MOVE' # MOVE Index\n",
    "}\n",
    "\n",
    "# ê° í‹°ì»¤ì— ëŒ€í•´ ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n",
    "for name, ticker in tickers.items():\n",
    "    try:\n",
    "        df = yf.download(ticker, start=start_date, end=end_date)\n",
    "        # ì¢…ê°€ë§Œ ì‚¬ìš©\n",
    "        df_final[name] = df['Close']\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {name}: {e}\")\n",
    "\n",
    "# MOVE ì¸ë±ìŠ¤ëŠ” ë³„ë„ë¡œ ì²˜ë¦¬í•´ì•¼ í•  ìˆ˜ ìˆìŒ (Bloomberg ë“±ì˜ ìœ ë£Œ ë°ì´í„° ì†ŒìŠ¤ í•„ìš”)\n",
    "print(\"Note: MOVE Index data needs to be obtained from a separate source like Bloomberg\")\n",
    "\n",
    "# ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
    "df_final = df_final.fillna(method='ffill')\n",
    "\n",
    "# ë°ì´í„° í™•ì¸\n",
    "print(\"\\nFirst few rows of the data:\")\n",
    "print(df_final.head())\n",
    "\n",
    "# ê¸°ë³¸ í†µê³„ í™•ì¸\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(df_final.describe())\n",
    "\n",
    "# CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "output_filename = '_economic_indicators_.csv'\n",
    "df_final.to_csv(output_filename)\n",
    "print(f\"\\nData saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ê²½ì œì§€í‘œ ë°ì´í„° ê²°ì¸¡ì¹˜ ì±„ìš°ê¸° "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¡œë“œ\n",
    "df = pd.read_csv('_economic_indicators_.csv', encoding='utf-8')\n",
    "\n",
    "# Date ì»¬ëŸ¼ì„ datetimeìœ¼ë¡œ ë³€í™˜\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# ì‹œì‘ì¼ê³¼ ë§ˆì§€ë§‰ì¼ ì¶”ì¶œ\n",
    "start_date = df['Date'].min()\n",
    "end_date = df['Date'].max()\n",
    "\n",
    "# ëª¨ë“  ë‚ ì§œê°€ í¬í•¨ëœ ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "all_dates = pd.DataFrame(\n",
    "    {'Date': pd.date_range(start_date, end_date, freq='D')}\n",
    ")\n",
    "\n",
    "# ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•©\n",
    "filled_df = pd.merge(all_dates, df, on='Date', how='left')\n",
    "\n",
    "# ê²°ì¸¡ì¹˜ë¥¼ ì´ì „ ê°’ìœ¼ë¡œ ì±„ìš°ê¸°\n",
    "filled_df = filled_df.ffill()\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "filled_df.to_csv('filled_economic_indicators.csv', index=False)\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸\n",
    "print(\"ì²˜ë¦¬ëœ ë°ì´í„° ìƒ˜í”Œ:\")\n",
    "print(filled_df.head())\n",
    "print(\"\\nê²°ì¸¡ì¹˜ í™•ì¸:\")\n",
    "print(filled_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### íŠ¸ë Œë“œ ë°ì´í„° ì–´ì¢…ë³„ë¡œ ê·¸ë£¹í™”\n",
    "\n",
    "merged_trends.csv ìƒì„±  \n",
    "nst_@@@ --> ê·¸ë£¹í™”_nst_@@@\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### í•¨ìˆ˜ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_age_groups(file_path):\n",
    "    # CSV íŒŒì¼ ì½ê¸°\n",
    "    df = pd.read_csv(file_path, encoding='utf-8')\n",
    "    \n",
    "    # ì—°ë ¹ëŒ€ ê·¸ë£¹ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ìƒì„±\n",
    "    age_mapping = {\n",
    "        '19_24': '20ëŒ€',\n",
    "        '25_29': '20ëŒ€',\n",
    "        '30_34': '30ëŒ€',\n",
    "        '35_39': '30ëŒ€',\n",
    "        '40_44': '40ëŒ€',\n",
    "        '45_49': '40ëŒ€',\n",
    "        '50_54': '50ëŒ€',\n",
    "        '55_59': '50ëŒ€',\n",
    "        '60_80': '60ëŒ€ ì´ìƒ'\n",
    "    }\n",
    "    \n",
    "    # ì›í•˜ëŠ” ì—°ë ¹ëŒ€ë§Œ í•„í„°ë§\n",
    "    df = df[df['age'].isin(age_mapping.keys())]\n",
    "    \n",
    "    # ìƒˆë¡œìš´ ì—°ë ¹ëŒ€ ì»¬ëŸ¼ ìƒì„±\n",
    "    df['age_group'] = df['age'].map(age_mapping)\n",
    "    \n",
    "    # ì¼ìë³„, ìƒˆë¡œìš´ ì—°ë ¹ëŒ€ë³„ë¡œ score í•©ì‚°\n",
    "    result = df.groupby(['date', 'name', 'age_group'])['score'].sum().reset_index()\n",
    "    \n",
    "    # í”¼ë²— í…Œì´ë¸”ë¡œ ë³€í™˜í•˜ì—¬ ë³´ê¸° ì¢‹ê²Œ ì •ë¦¬\n",
    "    pivot_result = result.pivot(index=['date', 'name'], \n",
    "                              columns='age_group', \n",
    "                              values='score').reset_index()\n",
    "    \n",
    "    # ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬\n",
    "    column_order = [ 'date', 'name', '20ëŒ€', '30ëŒ€', '40ëŒ€', '50ëŒ€', '60ëŒ€ ì´ìƒ']\n",
    "    pivot_result = pivot_result[column_order]\n",
    "    \n",
    "    return pivot_result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### íŒŒì¼ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Œ ìˆ˜ì‚°ë¬¼ ì¢…ë¥˜ë³„ íŒŒì¼ ê²½ë¡œ ë° ì´ë¦„ ì„¤ì •\n",
    "fish_files = {\n",
    "    'ê´‘ì–´': '../../data/raw/nst_ê´‘ì–´_trend_2025-01-17.csv',\n",
    "    'ë†ì–´': '../../data/raw/nst_ë†ì–´_trend_2025-01-17.csv',\n",
    "    'ëŒ€ê²Œ': '../../data/raw/nst_ëŒ€ê²Œ_trend_2025-01-17.csv',\n",
    "    'ë°©ì–´': '../../data/raw/nst_ë°©ì–´_trend_2025-01-17.csv',\n",
    "    'ì—°ì–´': '../../data/raw/nst_ì—°ì–´_trend_2025-01-17.csv',\n",
    "    'ìš°ëŸ­': '../../data/raw/nst_ìš°ëŸ­_trend_2025-01-17.csv',\n",
    "    'ì°¸ë”': '../../data/raw/nst_ì°¸ë”_trend_2025-01-17.csv'\n",
    "}\n",
    "\n",
    "# ğŸ“Œ ê°œë³„ íŒŒì¼ ì €ì¥ + ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì¶œë ¥ ì¤€ë¹„\n",
    "all_results = {}\n",
    "\n",
    "# ğŸ“Œ ëª¨ë“  íŒŒì¼ ì²˜ë¦¬\n",
    "for fish, file_path in fish_files.items():\n",
    "    try:\n",
    "        processed_df = process_age_groups(file_path, fish)\n",
    "\n",
    "        # ê°œë³„ íŒŒì¼ ì €ì¥\n",
    "        output_filename = f'ê·¸ë£¹í™”_nst_{fish}_trend_2025-01-17.csv'\n",
    "        processed_df.to_csv(output_filename, index=False)\n",
    "\n",
    "        # ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ (ì²« 3ê°œ í–‰ë§Œ ì €ì¥)\n",
    "        all_results[fish] = processed_df.head(3)\n",
    "\n",
    "        print(f\"âœ… '{output_filename}' ì €ì¥ ì™„ë£Œ!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing {fish}: {e}\")\n",
    "\n",
    "# ğŸ“Œ ìµœì¢… ì²˜ë¦¬ëœ ê²°ê³¼ ì¶œë ¥ (ê°ê° 3ê°œ í–‰ë§Œ ì¶œë ¥)\n",
    "print(\"\\nğŸ“Š ìµœì¢… ì²˜ë¦¬ëœ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸° (ê°ê° ìƒìœ„ 3ê°œ í–‰)\\n\")\n",
    "for fish, df_sample in all_results.items():\n",
    "    print(f\"ğŸ“Œ {fish} ë°ì´í„° ìƒ˜í”Œ:\")\n",
    "    print(df_sample)\n",
    "    print(\"-\" * 50)  # êµ¬ë¶„ì„  ì¶”ê°€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê¸°ìƒ ë°ì´í„° ì „ì²˜ë¦¬\n",
    "\n",
    "weatherdata_processed.csv ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•¨ìˆ˜ ì •ì˜\n",
    "\n",
    "def create_weather_columns(df, station_cols, output_path):\n",
    "   result_df = df.copy()\n",
    "   result_df['ì¼ì‹œ'] = pd.to_datetime(result_df['ì¼ì‹œ'])\n",
    "   \n",
    "   final_df = pd.DataFrame({'ì¼ì‹œ': result_df['ì¼ì‹œ'].unique()})\n",
    "   \n",
    "   for station, columns in station_cols.items():\n",
    "       station_data = result_df[result_df['ì§€ì '] == station].copy()\n",
    "       \n",
    "       for orig_col, new_col in columns:\n",
    "           station_col = station_data[['ì¼ì‹œ', orig_col]].copy()\n",
    "           final_df = pd.merge(final_df, station_col.rename(columns={orig_col: new_col}), \n",
    "                             on='ì¼ì‹œ', how='left')\n",
    "   \n",
    "   # ì¹¼ëŸ¼ì„ ê°€ë‚˜ë‹¤ìˆœìœ¼ë¡œ ì •ë ¬ ('ì¼ì‹œ' ì»¬ëŸ¼ì€ ì²«ë²ˆì§¸ë¡œ ìœ ì§€)\n",
    "   sorted_cols = ['ì¼ì‹œ'] + sorted([col for col in final_df.columns if col != 'ì¼ì‹œ'])\n",
    "   result = final_df[sorted_cols].sort_values('ì¼ì‹œ')\n",
    "   \n",
    "   result.to_csv(output_path, index=False, encoding='utf-8')\n",
    "   return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV íŒŒì¼ ì½ê¸°\n",
    "df = pd.read_csv('forecast_agg.csv', encoding='utf-8')\n",
    "\n",
    "# í”¼ì³ ì„ íƒ\n",
    "station_columns = {\n",
    "\t22105: [\n",
    "\t\t('ê¸°ì˜¨', 'ê´‘ì–´_ê¸°ì˜¨_22105'),\n",
    "\t\t('ê¸°ì˜¨', 'ë†ì–´_ê¸°ì˜¨_22105'),\n",
    "\t\t('ê¸°ì˜¨', 'ëŒ€ê²Œ_ê¸°ì˜¨_22105'),\n",
    "\t\t('íŒŒì£¼ê¸°', 'ëŒ€ê²Œ_íŒŒì£¼ê¸°_22105'),\n",
    "\t\t('íŒŒì£¼ê¸°', 'ë°©ì–´_íŒŒì£¼ê¸°_22105'),\n",
    "\t\t('ê¸°ì˜¨', 'ì—°ì–´_ê¸°ì˜¨_22105'),\n",
    "\t\t('ìŠµë„', 'ì—°ì–´_ìŠµë„_22105')\n",
    "\t],\n",
    "    \n",
    "\t22107: [\n",
    "\t\t('ìˆ˜ì˜¨', 'ê´‘ì–´_ìˆ˜ì˜¨_22107'),\n",
    "\t\t('ìˆ˜ì˜¨', 'ë†ì–´_ìˆ˜ì˜¨_22107'),\n",
    "\t\t('ìˆ˜ì˜¨', 'ë°©ì–´_ìˆ˜ì˜¨_22107'),\n",
    "\t\t('ìˆ˜ì˜¨', 'ì—°ì–´_ìˆ˜ì˜¨_22107'),\n",
    "\t\t('ìˆ˜ì˜¨', 'ì°¸ë”_ìˆ˜ì˜¨_22107')\n",
    "\t],\n",
    "\n",
    "\t22186: [\n",
    "\t\t('ìŠµë„', 'ê´‘ì–´_ìŠµë„_22186'),\n",
    "\t\t('ìŠµë„', 'ë†ì–´_ìŠµë„_22186'),\n",
    "\t\t('ê¸°ì˜¨', 'ìš°ëŸ­_ê¸°ì˜¨_22186'),\n",
    "\t\t('ìˆ˜ì˜¨', 'ìš°ëŸ­_ìˆ˜ì˜¨_22186')\n",
    "\t\t],\n",
    "        \n",
    "\t22188: [\n",
    "\t\t('ìˆ˜ì˜¨', 'ëŒ€ê²Œ_ìˆ˜ì˜¨_22188'),\n",
    "\t\t('ìŠµë„', 'ëŒ€ê²Œ_ìŠµë„_22188')\n",
    "\t\t],        \n",
    "\n",
    "\t22189: [\n",
    "\t\t('íŒŒì£¼ê¸°', 'ìš°ëŸ­_íŒŒì£¼ê¸°_22189')\n",
    "\t\t],       \n",
    "\n",
    "\t22190: [\n",
    "\t\t('íŒŒì£¼ê¸°', 'ê´‘ì–´_íŒŒì£¼ê¸°_22190'),\n",
    "        ('íŒŒì£¼ê¸°', 'ë†ì–´_íŒŒì£¼ê¸°_22190'),\n",
    "        ('ê¸°ì˜¨', 'ë°©ì–´_ê¸°ì˜¨_22190'),\n",
    "        ('ìŠµë„', 'ë°©ì–´_ìŠµë„_22190'),\n",
    "        ('íŒŒì£¼ê¸°', 'ì—°ì–´_íŒŒì£¼ê¸°_22190'),\n",
    "        ('ìŠµë„', 'ìš°ëŸ­_ìŠµë„_22190'),\n",
    "        ('ê¸°ì˜¨', 'ì°¸ë”_ê¸°ì˜¨_22190'),\n",
    "        ('ìŠµë„', 'ì°¸ë”_ìŠµë„_22190'),\n",
    "        ('íŒŒì£¼ê¸°', 'ì°¸ë”_íŒŒì£¼ê¸°_22190')\n",
    "\t\t]  \n",
    "\n",
    "\t}\n",
    "\n",
    "result = create_weather_columns(df, station_columns, 'weatherdata_processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë³€ìˆ˜ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°\n",
    "\n",
    "merged_all_data.csv ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_fish_trends(file_paths, output_path):\n",
    "   # ì²« ë²ˆì§¸ íŒŒì¼ ë¡œë“œ ë° name ì»¬ëŸ¼ ì œê±°\n",
    "   merged_df = pd.read_csv(file_paths[0], encoding='utf-8')\n",
    "   merged_df = merged_df.drop('name', axis=1)\n",
    "   merged_df['date'] = pd.to_datetime(merged_df['date'])\n",
    "   \n",
    "   # ë‚˜ë¨¸ì§€ íŒŒì¼ë“¤ ë³‘í•©\n",
    "   for file_path in file_paths[1:]:\n",
    "       df = pd.read_csv(file_path, encoding='utf-8')\n",
    "       df = df.drop('name', axis=1)\n",
    "       df['date'] = pd.to_datetime(df['date'])\n",
    "       merged_df = pd.merge(merged_df, df, on='date', how='outer')\n",
    "   \n",
    "   # ë‚ ì§œìˆœ ì •ë ¬\n",
    "   result = merged_df.sort_values('date')\n",
    "   \n",
    "   # CSV ì €ì¥\n",
    "   result.to_csv(output_path, index=False, encoding='utf-8')\n",
    "   return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‚¬ìš© ì˜ˆì‹œ:\n",
    "files = [\n",
    "    'ê·¸ë£¹í™”_nst_ê´‘ì–´_trend_2025-01-17.csv', \n",
    "    'ê·¸ë£¹í™”_nst_ë†ì–´_trend_2025-01-17.csv', \n",
    "    'ê·¸ë£¹í™”_nst_ëŒ€ê²Œ_trend_2025-01-17.csv', \n",
    "    'ê·¸ë£¹í™”_nst_ë°©ì–´_trend_2025-01-17.csv', \n",
    "    'ê·¸ë£¹í™”_nst_ì—°ì–´_trend_2025-01-17.csv', \n",
    "    'ê·¸ë£¹í™”_nst_ìš°ëŸ­_trend_2025-01-17.csv', \n",
    "    'ê·¸ë£¹í™”_nst_ì°¸ë”_trend_2025-01-17.csv'\n",
    "    ]\n",
    "\n",
    "result = merge_fish_trends(files, 'merged_trends.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data(file_paths, output_path):\n",
    "   # ì²« ë²ˆì§¸ íŒŒì¼ ë¡œë“œ ë° name ì»¬ëŸ¼ ì œê±°\n",
    "   merged_df = pd.read_csv(file_paths[0], encoding='utf-8')\n",
    "   merged_df = merged_df.drop('name', axis=1)\n",
    "   merged_df['date'] = pd.to_datetime(merged_df['date'])\n",
    "   \n",
    "   # ë‚˜ë¨¸ì§€ íŒŒì¼ë“¤ ë³‘í•©\n",
    "   for file_path in file_paths[1:]:\n",
    "       df = pd.read_csv(file_path, encoding='utf-8')\n",
    "       df = df.drop('name', axis=1)\n",
    "       df['date'] = pd.to_datetime(df['date'])\n",
    "       merged_df = pd.merge(merged_df, df, on='date', how='outer')\n",
    "   \n",
    "   # ë‚ ì§œìˆœ ì •ë ¬\n",
    "   result = merged_df.sort_values('date')\n",
    "   \n",
    "   # CSV ì €ì¥\n",
    "   result.to_csv(output_path, index=False, encoding='utf-8')\n",
    "   return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”ì—†ëŠ” ì¹¼ëŸ¼ ì œê±°\n",
    "\n",
    "df_ê´‘ì–´ = pd.read_csv('item_price_lag_filled.csv')\n",
    "df_ê´‘ì–´ = df_ê´‘ì–´.drop('minPrice', axis=1)\n",
    "df_ê´‘ì–´.to_csv('item_price_lag_filled.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_data(output_path='merged_all_data.csv'):\n",
    "   # ê° íŒŒì¼ ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "   \n",
    "   economic = pd.read_csv('_filled_economic_indicators.csv', encoding='utf-8')\n",
    "   economic['Date'] = pd.to_datetime(economic['Date'])\n",
    "   economic = economic.rename(columns={'Date': 'ë‚ ì§œ'})\n",
    "   \n",
    "   trends = pd.read_csv('merged_trends.csv', encoding='utf-8')\n",
    "   trends['date'] = pd.to_datetime(trends['date'])\n",
    "   trends = trends.rename(columns={'date': 'ë‚ ì§œ'})\n",
    "   \n",
    "   weather = pd.read_csv('_weatherdata_processed.csv', encoding='utf-8')\n",
    "   weather['date'] = pd.to_datetime(weather['date'])\n",
    "   weather = weather.rename(columns={'date': 'ë‚ ì§œ'})\n",
    "   \n",
    "   # ëª¨ë“  ë‚ ì§œ ì¶”ì¶œ\n",
    "   all_dates = pd.concat([\n",
    "        economic['ë‚ ì§œ'], \n",
    "        trends['ë‚ ì§œ'], \n",
    "       weather['ë‚ ì§œ']\n",
    "   ]).unique()\n",
    "   \n",
    "   # ë‚ ì§œ ê¸°ì¤€ ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "   date_df = pd.DataFrame({'ë‚ ì§œ': all_dates})\n",
    "   date_df = date_df.sort_values('ë‚ ì§œ')\n",
    "   \n",
    "   # ë°ì´í„° ë³‘í•©\n",
    "   dfs = [\n",
    "\t\tdate_df, \n",
    "\t\teconomic, \n",
    "\t\ttrends,\n",
    "\t\tweather\n",
    "   ]\n",
    "   \n",
    "   result = dfs[0]\n",
    "   for df in dfs[1:]:\n",
    "       result = pd.merge(result, df, on='ë‚ ì§œ', how='left')\n",
    "   \n",
    "   # ê²°ê³¼ ì €ì¥\n",
    "   result.to_csv(output_path, index=False, encoding='utf-8')\n",
    "   return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = merge_all_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°\n",
    "\n",
    " \"filled_merged_all_data.csv\" ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def fill_missing_values(file_path, output_path):\n",
    "    \"\"\"\n",
    "    CSV íŒŒì¼ì—ì„œ ê²°ì¸¡ì¹˜ë¥¼ `ffill`ì„ ì‚¬ìš©í•˜ì—¬ ì±„ìš°ë˜, ì´ì „ ê°’ì´ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë‘ .\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): ì…ë ¥ CSV íŒŒì¼ ê²½ë¡œ.\n",
    "        output_path (str): ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ì €ì¥í•  ê²½ë¡œ.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: ê²°ì¸¡ì¹˜ê°€ ì±„ì›Œì§„ ë°ì´í„°í”„ë ˆì„.\n",
    "    \"\"\"\n",
    "    \n",
    "    # CSV íŒŒì¼ ë¡œë“œ\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # ê²°ì¸¡ì¹˜ í™•ì¸ (ì²˜ë¦¬ ì „)\n",
    "    missing_before = df.isnull().sum()\n",
    "\n",
    "    # ê²°ì¸¡ì¹˜ `ffill`ë¡œ ì±„ìš°ê¸° (ì´ì „ ê°’ì´ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë‘ )\n",
    "    df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "    # ê²°ì¸¡ì¹˜ í™•ì¸ (ì²˜ë¦¬ í›„)\n",
    "    missing_after = df.isnull().sum()\n",
    "\n",
    "    # ë³€ê²½ëœ ê²°ì¸¡ì¹˜ ê°œìˆ˜ ì¶œë ¥\n",
    "    missing_summary = pd.DataFrame({'Before': missing_before, 'After': missing_after})\n",
    "    print(\"\\nğŸ” ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì „í›„ ë¹„êµ:\")\n",
    "    print(missing_summary)\n",
    "\n",
    "    # ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"\\nâœ… ì²˜ë¦¬ëœ ë°ì´í„°ê°€ '{output_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì „í›„ ë¹„êµ:\n",
      "                Before  After\n",
      "ë‚ ì§œ                   0      0\n",
      "í™œì—°ì–´_ê±°ë˜ëŸ‰(í†¤)        1107   1097\n",
      "í™œì—°ì–´_ê°€ê²©(NOK/kg)    1107   1097\n",
      "ê´‘ì–´_ëŒ€              1938   1098\n",
      "ê´‘ì–´_ì¤‘              1938   1098\n",
      "...                ...    ...\n",
      "ìš°ëŸ­_íŒŒì£¼ê¸°_22189       487    355\n",
      "ì°¸ë”_ê¸°ì˜¨_22190        452    342\n",
      "ì°¸ë”_ìˆ˜ì˜¨_22107        203      0\n",
      "ì°¸ë”_ìŠµë„_22190        454    342\n",
      "ì°¸ë”_íŒŒì£¼ê¸°_22190       414    342\n",
      "\n",
      "[122 rows x 2 columns]\n",
      "\n",
      "âœ… ì²˜ë¦¬ëœ ë°ì´í„°ê°€ 'filled_merged_all_data.csv'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_38656\\1915651589.py:22: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# âœ… ì‹¤í–‰ ì˜ˆì‹œ\n",
    "input_file = \"../../data/features/merged_all_data.csv\"  # ì›ë³¸ ë°ì´í„° ê²½ë¡œ\n",
    "output_file = \"filled_merged_all_data.csv\"  # ì €ì¥ë  íŒŒì¼ ê²½ë¡œ\n",
    "\n",
    "# í•¨ìˆ˜ ì‹¤í–‰\n",
    "filled_df = fill_missing_values(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì–´ì¢…ë³„ ê°€ê²© ë°ì´í„° íŒŒì¼ ë¶„ë¦¬\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV íŒŒì¼ ì½ê¸°\n",
    "df = pd.read_csv('pp/item_price_lag_filled.csv')\n",
    "\n",
    "# ìœ ë‹ˆí¬í•œ item ëª©ë¡ í™•ì¸\n",
    "unique_items = df['item'].unique()\n",
    "\n",
    "# ê° itemë³„ë¡œ ë°ì´í„° ë¶„ë¦¬í•˜ì—¬ ì €ì¥\n",
    "for item in unique_items:\n",
    "    # í•´ë‹¹ itemì˜ ë°ì´í„°ë§Œ í•„í„°ë§\n",
    "    item_df = df[df['item'] == item]\n",
    "    \n",
    "    # íŒŒì¼ëª…ì— ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ item ì´ë¦„ ì²˜ë¦¬ (íŠ¹ìˆ˜ë¬¸ì ë“± ì œê±°)\n",
    "    file_name = f\"{item.replace(' ', '_').replace('/', '_')}_price_data.csv\"\n",
    "    \n",
    "    # CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "    item_df.to_csv(file_name, index=False, encoding='utf-8')\n",
    "    \n",
    "    # ì €ì¥ ì™„ë£Œ ë©”ì‹œì§€ ì¶œë ¥\n",
    "    print(f\"{item} ë°ì´í„°ê°€ {file_name}ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    print(f\"í–‰ ìˆ˜: {len(item_df)}\")\n",
    "\n",
    "# ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ ë©”ì‹œì§€\n",
    "print(\"\\nì „ì²´ í’ˆëª© ë¶„ë¦¬ ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "print(f\"ì´ {len(unique_items)}ê°œì˜ íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë³€ìˆ˜ë‘ ì¸ì–´êµì£¼í•´ì ë‹¨ ê°€ê²©ì´ë‘ í•©ì¹˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### í•©ì¹œ ë°ì´í„°ì— íƒ€ì„ë˜ê·¸ ì ìš©í•˜ê¸° "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env311_cu121",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
